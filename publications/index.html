<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="pBssdi5JbWwZ48hVHfPQqDlzvzHALeReZlHBapJhCgY"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Publications | Jekaterina Novikova </title> <meta name="author" content="Jekaterina Novikova"> <meta name="description" content="Select publications only. See my Google Scholar profile for a full list of publications."> <meta name="keywords" content="jekaterina-novikova, research, ai, nlp, llm"> <meta property="og:site_name" content="Jekaterina Novikova"> <meta property="og:type" content="website"> <meta property="og:title" content="Jekaterina Novikova | Publications"> <meta property="og:url" content="https://jeknov.github.io/publications/"> <meta property="og:description" content="Select publications only. See my Google Scholar profile for a full list of publications."> <meta property="og:image" content="true"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Publications"> <meta name="twitter:description" content="Select publications only. See my Google Scholar profile for a full list of publications."> <meta name="twitter:image" content="true"> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Jekaterina Novikova",
            "givenName": "Jekaterina",
            "familyName": "Novikova",
            "alternateName": [
                "J. Novikova",
                "Jekaterina Novikova, PhD"
              ],
            "jobTitle": "AI Research Scientist",
            "affiliation": {
                "@type": "Organization",
                "name": "Vanguard"
              },
            "knowsAbout":[
                "Artificial Intelligence",
                "AI",
                "Natural Language Processing",
                "NLP",
                "Generative AI",
                "GenAI",
                "LLM",
                "Large Language Models"
              ],
            "description": "Jekaterina Novikova is a Principal AI Research Scientist specializing in natural language processing, trustworthy AI, and human-robot interaction."
        },
        "url": "https://jeknov.github.io/publications/",
        "@type": "WebSite",
        "description": "Select publications only. See my Google Scholar profile for a full list of publications.",
        "headline": "Publications",
        
        "sameAs": ["https://scholar.google.com/citations?user=C75JskwAAAAJ","https://www.linkedin.com/in/jnovikova","https://www.semanticscholar.org/author/Jekaterina-Novikova/2848048","https://www.researchgate.net/profile/Jekaterina-Novikova-2/","https://orcid.org/0000-0003-4754-6126","https://bsky.app/profile/j-novikova-nlp.bsky.social","https://twitter.com/J_Novikova_NLP","https://github.com/jeknov","https://dl.acm.org/profile/87358850257/","https://ieeexplore.ieee.org/author/37085694440/"],
        
        "name": "Jekaterina Novikova",
        "@context": "https://schema.org"
    }
  </script> <script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": ["VideoObject", "PodcastEpisode"],
  "name": "Episode 10: Open Science and LLMs",
  "description": "Can open-source large language models really outperform closed ones like Claude 3.5? In this episode of the Women in AI Research podcast, Jekaterina Novikova and Malikeh Ehghaghi engage with Valentina Pyatkin, a postdoctoral researcher at the Allen Institute for AI. We dive deep into the future of open science, LLM research, and extending model capabilities.",
  "thumbnailUrl": "https://jeknov.github.io/assets/img/yt-Episode10.png",
  "uploadDate": "2025-09-17",
  "duration": "PT1H11M38S",
  "contentUrl": "https://youtu.be/DPhqm1EHxtU?si=8iWxQm-thtjrofkJ",
  "sameAs": "https://www.youtube.com/@WomeninAIResearch",
  "episodeNumber": 10,
  "isPartOf": {
    "@type": "PodcastSeries",
    "name": "Women in AI Research",
    "description": "Women in AI Research WiAIR is a podcast dedicated to celebrating the remarkable contributions of female AI researchers from around the globe. Our mission is to challenge the prevailing perception that AI research is predominantly male-driven.",
    "url": "https://women-in-ai-research.github.io",
    "publisher": {
      "@type": "Organization",
      "name": "Women in AI Research",
      "url": "https://women-in-ai-research.github.io"
    },
    "sameAs": [
      "https://www.youtube.com/@WomeninAIResearch",
      "https://open.spotify.com/show/51RJNlZarFTJXXIlz5Qx3M",
      "https://podcasts.apple.com/ca/podcast/women-in-ai-research-wiair/id1805993416"
    ]
  },
  "actor": {
    "@type": "Person",
    "name": "Valentina Pyatkin",
    "sameAs": "https://valentinapy.github.io"
  },
  "creator": [
    {
      "@type": "Person",
      "name": "Jekaterina Novikova",
      "givenName": "Jekaterina",
      "familyName": "Novikova",
      "alternateName": [
        "J. Novikova",
        "Jekaterina Novikova, PhD"
      ],
      "sameAs": [
        "https://scholar.google.ca/citations?user=C75JskwAAAAJ",
        "https://www.linkedin.com/in/jekaterina-novikova",
        "https://www.semanticscholar.org/author/Jekaterina-Novikova/2848048",
        "https://www.researchgate.net/profile/Jekaterina-Novikova-2",
        "https://x.com/J_Novikova_NLP",
        "https://bsky.app/profile/j-novikova-nlp.bsky.social",
        "https://orcid.org/0000-0003-4754-6126",
        "https://openreview.net/profile?id=~Jekaterina_Novikova1",
        "https://ieeexplore.ieee.org/author/37085694440",
        "https://huggingface.co/Jekaterina",
        "https://aclanthology.org/people/jekaterina-novikova/",
        "https://github.com/jnovikova",
        "https://loop.frontiersin.org/people/1157893/overview"
      ]
    },
    {
      "@type": "Person",
      "name": "Malikeh Ehghaghi",
      "sameAs": "https://www.linkedin.com/in/malikeh97"
    }
  ]
}
</script> <script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": ["VideoObject", "PodcastEpisode"],
  "name": "Ep.9: Unlocking LLM Reasoning, with Simeng Sophia Han",
  "description": "In this episode of the Women in AI Research podcast, we explore the future of ùêãùêãùêå ùê´ùêûùêöùê¨ùê®ùêßùê¢ùêßùê†, ùêûùêØùêöùê•ùêÆùêöùê≠ùê¢ùê®ùêß, ùêöùêßùêù ùêûùê±ùê©ùê•ùêöùê¢ùêßùêöùêõùê•ùêû ùêÄùêà. You will learn why evaluating reasoning goes beyond correctness, how brain teasers uncover hidden strengths and weaknesses of LLMs, why consistency in AI outputs is essential for building trust, and many other important topics.",
  "thumbnailUrl": "assets/img/yt-Episode9.png",
  "uploadDate": "2025-08-27",
  "duration": "PT53M39S",
  "contentUrl": "https://youtu.be/IalFROkCH0c",
  "sameAs": "https://www.youtube.com/@WomeninAIResearch",
  "episodeNumber": 9,
  "isPartOf": {
    "@type": "PodcastSeries",
    "name": "Women in AI Research",
    "description": "Women in AI Research WiAIR is a podcast dedicated to celebrating the remarkable contributions of female AI researchers from around the globe. Our mission is to challenge the prevailing perception that AI research is predominantly male-driven.",
    "url": "https://women-in-ai-research.github.io",
    "publisher": {
      "@type": "Organization",
      "name": "Women in AI Research",
      "url": "https://women-in-ai-research.github.io"
    },
    "sameAs": [
      "https://www.youtube.com/@WomeninAIResearch",
      "https://open.spotify.com/show/51RJNlZarFTJXXIlz5Qx3M",
      "https://podcasts.apple.com/ca/podcast/women-in-ai-research-wiair/id1805993416"
    ]
  },
  "actor": {
    "@type": "Person",
    "name": "Simeng Sophia Han",
    "sameAs": "https://sophiahan6.github.io/"
  },
  "creator": [
    {
      "@type": "Person",
      "name": "Jekaterina Novikova",
      "givenName": "Jekaterina",
      "familyName": "Novikova",
      "alternateName": [
        "J. Novikova",
        "Jekaterina Novikova, PhD"
      ],
      "sameAs": [
        "https://scholar.google.ca/citations?user=C75JskwAAAAJ",
        "https://www.linkedin.com/in/jekaterina-novikova",
        "https://www.semanticscholar.org/author/Jekaterina-Novikova/2848048",
        "https://www.researchgate.net/profile/Jekaterina-Novikova-2",
        "https://x.com/J_Novikova_NLP",
        "https://bsky.app/profile/j-novikova-nlp.bsky.social",
        "https://orcid.org/0000-0003-4754-6126",
        "https://openreview.net/profile?id=~Jekaterina_Novikova1",
        "https://ieeexplore.ieee.org/author/37085694440",
        "https://huggingface.co/Jekaterina",
        "https://aclanthology.org/people/jekaterina-novikova/",
        "https://github.com/jnovikova",
        "https://loop.frontiersin.org/people/1157893/overview"
      ]
    },
    {
      "@type": "Person",
      "name": "Malikeh Ehghaghi",
      "sameAs": "https://www.linkedin.com/in/malikeh97"
    }
  ]
}
</script> <script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": ["VideoObject", "PodcastEpisode"],
  "name": "Ep.8: LLM Hallucinations and Machine Unlearning, with Dr. Abhilasha Ravichander",
  "description": "In this episode of the Women in AI Research podcast, we explore the future of ùêãùêãùêå ùê´ùêûùêöùê¨ùê®ùêßùê¢ùêßùê†, ùêûùêØùêöùê•ùêÆùêöùê≠ùê¢ùê®ùêß, ùêöùêßùêù ùêûùê±ùê©ùê•ùêöùê¢ùêßùêöùêõùê•ùêû ùêÄùêà. You will learn why evaluating reasoning goes beyond correctness, how brain teasers uncover hidden strengths and weaknesses of LLMs, why consistency in AI outputs is essential for building trust, and many other important topics.",
  "thumbnailUrl": "assets/img/yt-Episode8.png",
  "uploadDate": "2025-08-06",
  "duration": "PT1H03M21S",
  "contentUrl": "https://youtu.be/QPp0cJNBbL8",
  "sameAs": "https://www.youtube.com/@WomeninAIResearch",
  "episodeNumber": 8,
  "isPartOf": {
    "@type": "PodcastSeries",
    "name": "Women in AI Research",
    "description": "Women in AI Research WiAIR is a podcast dedicated to celebrating the remarkable contributions of female AI researchers from around the globe. Our mission is to challenge the prevailing perception that AI research is predominantly male-driven.",
    "url": "https://women-in-ai-research.github.io",
    "publisher": {
      "@type": "Organization",
      "name": "Women in AI Research",
      "url": "https://women-in-ai-research.github.io"
    },
    "sameAs": [
      "https://www.youtube.com/@WomeninAIResearch",
      "https://open.spotify.com/show/51RJNlZarFTJXXIlz5Qx3M",
      "https://podcasts.apple.com/ca/podcast/women-in-ai-research-wiair/id1805993416"
    ]
  },
  "actor": {
    "@type": "Person",
    "name": "Abhilasha Ravichander",
    "sameAs": "https://lasharavichander.github.io/"
  },
  "creator": [
    {
      "@type": "Person",
      "name": "Jekaterina Novikova",
      "givenName": "Jekaterina",
      "familyName": "Novikova",
      "alternateName": [
        "J. Novikova",
        "Jekaterina Novikova, PhD"
      ],
      "sameAs": [
        "https://scholar.google.ca/citations?user=C75JskwAAAAJ",
        "https://www.linkedin.com/in/jekaterina-novikova",
        "https://www.semanticscholar.org/author/Jekaterina-Novikova/2848048",
        "https://www.researchgate.net/profile/Jekaterina-Novikova-2",
        "https://x.com/J_Novikova_NLP",
        "https://bsky.app/profile/j-novikova-nlp.bsky.social",
        "https://orcid.org/0000-0003-4754-6126",
        "https://openreview.net/profile?id=~Jekaterina_Novikova1",
        "https://ieeexplore.ieee.org/author/37085694440",
        "https://huggingface.co/Jekaterina",
        "https://aclanthology.org/people/jekaterina-novikova/",
        "https://github.com/jnovikova",
        "https://loop.frontiersin.org/people/1157893/overview"
      ]
    },
    {
      "@type": "Person",
      "name": "Malikeh Ehghaghi",
      "sameAs": "https://www.linkedin.com/in/malikeh97"
    }
  ]
}
</script> <script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "ScholarlyArticle",
  "headline": "Why We Need New Evaluation Metrics for NLG",
  "name": "Why We Need New Evaluation Metrics for NLG",
  "author": [
    {
      "@type": "Person",
      "name": "Jekaterina Novikova",
      "givenName": "Jekaterina",
      "familyName": "Novikova",
      "alternateName": [
        "J. Novikova",
        "Jekaterina Novikova, PhD"
      ],
      "sameAs": [
        "https://scholar.google.ca/citations?user=C75JskwAAAAJ",
        "https://www.linkedin.com/in/jekaterina-novikova",
        "https://www.semanticscholar.org/author/Jekaterina-Novikova/2848048",
        "https://www.researchgate.net/profile/Jekaterina-Novikova-2",
        "https://x.com/J_Novikova_NLP",
        "https://bsky.app/profile/j-novikova-nlp.bsky.social",
        "https://orcid.org/0000-0003-4754-6126",
        "https://openreview.net/profile?id=~Jekaterina_Novikova1",
        "https://ieeexplore.ieee.org/author/37085694440",
        "https://huggingface.co/Jekaterina",
        "https://aclanthology.org/people/jekaterina-novikova/",
        "https://github.com/jnovikova",
        "https://loop.frontiersin.org/people/1157893/overview"
      ]
    },
    {
      "@type": "Person",
      "name": "Ond≈ôej Du≈°ek"
    },
    {
      "@type": "Person",
      "name": "Amanda Cercas Curry"
    },
    {
      "@type": "Person",
      "name": "Verena Rieser"
    }
  ],
  "datePublished": "2017-09-15",
  "publisher": {
    "@type": "Organization",
    "name": "EMNLP"
  },
  "identifier": {
    "@type": "PropertyValue",
    "propertyID": "DOI",
    "value": "10.18653/v1/D17-1238"
  },
  "sameAs": "https://doi.org/10.18653/v1/D17-1238",
  "url": "https://aclanthology.org/D17-1238/",
  "isPartOf": {
    "@type": "PublicationVolume",
    "name": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    "url": "https://aclanthology.org/events/emnlp-2017/"
  },
  "description": "The majority of NLG evaluation relies on automatic metrics, such as BLEU. In this paper, we motivate the need for novel, system- and data-independent automatic evaluation methods: We investigate a wide range of metrics, including state-of-the-art word-based and novel grammar-based ones, and demonstrate that they only weakly reflect human judgements of system outputs as generated by data-driven, end-to-end NLG. We also show that metric performance is data- and system-specific. Nevertheless, our results also suggest that automatic metrics perform reliably at system-level and can support system development by finding cases where a system performs poorly."
}
</script> <script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "ScholarlyArticle",
  "headline": "The E2E Dataset: New Challenges For End-to-End Generation",
  "name": "The E2E Dataset: New Challenges For End-to-End Generation",
  "author": [
    {
      "@type": "Person",
      "name": "Jekaterina Novikova",
      "givenName": "Jekaterina",
      "familyName": "Novikova",
      "alternateName": [
        "J. Novikova",
        "Jekaterina Novikova, PhD"
      ],
      "sameAs": [
        "https://scholar.google.ca/citations?user=C75JskwAAAAJ",
        "https://www.linkedin.com/in/jekaterina-novikova",
        "https://www.semanticscholar.org/author/Jekaterina-Novikova/2848048",
        "https://www.researchgate.net/profile/Jekaterina-Novikova-2",
        "https://x.com/J_Novikova_NLP",
        "https://bsky.app/profile/j-novikova-nlp.bsky.social",
        "https://orcid.org/0000-0003-4754-6126",
        "https://openreview.net/profile?id=~Jekaterina_Novikova1",
        "https://ieeexplore.ieee.org/author/37085694440",
        "https://huggingface.co/Jekaterina",
        "https://aclanthology.org/people/jekaterina-novikova/",
        "https://github.com/jnovikova",
        "https://loop.frontiersin.org/people/1157893/overview"
      ]
    },
    {
      "@type": "Person",
      "name": "Ond≈ôej Du≈°ek"
    },
    {
      "@type": "Person",
      "name": "Verena Rieser"
    }
  ],
  "datePublished": "2017-08-15",
  "publisher": {
    "@type": "Organization",
    "name": "SIGdial"
  },
  "identifier": {
    "@type": "PropertyValue",
    "propertyID": "DOI",
    "value": "10.18653/v1/W17-5525"
  },
  "sameAs": "https://doi.org/10.18653/v1/W17-5525",
  "url": "https://aclanthology.org/W17-5525/",
  "isPartOf": {
    "@type": "PublicationVolume",
    "name": "Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue",
    "url": "https://aclanthology.org/events/sigdial-2017/"
  },
  "description": "This paper describes the E2E data, a new dataset for training end-to-end, data-driven natural language generation systems in the restaurant domain, which is ten times bigger than existing, frequently used datasets in this area. The E2E dataset poses new challenges: (1) its human reference texts show more lexical richness and syntactic variation, including discourse phenomena; (2) generating from this set requires content selection. As such, learning from this dataset promises more natural, varied and less template-like system utterances. We also establish a baseline on this dataset, which illustrates some of the difficulties associated with this data."
}
</script> <script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "ScholarlyArticle",
  "headline": "To BERT or not to BERT: Comparing Speech and Language-Based Approaches for Alzheimer's Disease Detection",
  "author": [
    {
      "@type": "Person",
      "name": "Jekaterina Novikova",
      "givenName": "Jekaterina",
      "familyName": "Novikova",
      "alternateName": [
        "J. Novikova",
        "Jekaterina Novikova, PhD"
      ],
      "sameAs": [
        "https://scholar.google.ca/citations?user=C75JskwAAAAJ",
        "https://www.linkedin.com/in/jekaterina-novikova",
        "https://www.semanticscholar.org/author/Jekaterina-Novikova/2848048",
        "https://www.researchgate.net/profile/Jekaterina-Novikova-2",
        "https://x.com/J_Novikova_NLP",
        "https://bsky.app/profile/j-novikova-nlp.bsky.social",
        "https://orcid.org/0000-0003-4754-6126",
        "https://openreview.net/profile?id=~Jekaterina_Novikova1",
        "https://ieeexplore.ieee.org/author/37085694440",
        "https://huggingface.co/Jekaterina",
        "https://aclanthology.org/people/jekaterina-novikova/",
        "https://github.com/jnovikova",
        "https://loop.frontiersin.org/people/1157893/overview"
      ]
    },
    {
      "@type": "Person",
      "name": "Aparna Balagopalan"
    },
    {
      "@type": "Person",
      "name": "Benjamin Eyre"
    },
    {
      "@type": "Person",
      "name": "Frank Rudzicz"
    }
  ],
  "datePublished": "2020-01-15",
  "publisher": {
    "@type": "Organization",
    "name": "Interspeech"
  },
  "identifier": {
    "@type": "PropertyValue",
    "propertyID": "DOI",
    "value": "10.21437/Interspeech.2020-2557"
  },
  "url": "https://www.isca-archive.org/interspeech_2020/balagopalan20_interspeech.html",
  "description": "Research related to automatically detecting Alzheimer's disease (AD) is important, given the high prevalence of AD and the high cost of traditional methods. Since AD significantly affects the content and acoustics of spontaneous speech, natural language processing and machine learning provide promising techniques for reliably detecting AD. We compare and contrast the performance of two such approaches for AD detection on the recent ADReSS challenge dataset: 1. using domain knowledge-based hand-crafted features that capture linguistic and acoustic phenomena, and 2. fine-tuning Bidirectional Encoder Representations from Transformer (BERT)-based sequence classification models. We also compare multiple feature-based regression models for a neuropsychological score task in the challenge. We observe that fine-tuned BERT models, given the relative importance of linguistics in cognitive impairment detection, outperform feature-based approaches on the AD detection task."
}
</script> <script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "ScholarlyArticle",
  "headline": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model",
  "author": [
    {
      "@type": "Person",
      "name": "Jekaterina Novikova",
      "givenName": "Jekaterina",
      "familyName": "Novikova",
      "alternateName": [
        "J. Novikova",
        "Jekaterina Novikova, PhD"
      ],
      "sameAs": [
        "https://scholar.google.ca/citations?user=C75JskwAAAAJ",
        "https://www.linkedin.com/in/jekaterina-novikova",
        "https://www.semanticscholar.org/author/Jekaterina-Novikova/2848048",
        "https://www.researchgate.net/profile/Jekaterina-Novikova-2",
        "https://x.com/J_Novikova_NLP",
        "https://bsky.app/profile/j-novikova-nlp.bsky.social",
        "https://orcid.org/0000-0003-4754-6126",
        "https://openreview.net/profile?id=~Jekaterina_Novikova1",
        "https://ieeexplore.ieee.org/author/37085694440",
        "https://huggingface.co/Jekaterina",
        "https://aclanthology.org/people/jekaterina-novikova/",
        "https://github.com/jnovikova",
        "https://loop.frontiersin.org/people/1157893/overview"
      ]
    },
    { "@type": "Person", "name": "Ellie Pavlick" },
    { "@type": "Person", "name": "Alexander M. Rush" },
    { "@type": "Person", "name": "Stella Biderman" },
    { "@type": "Person", "name": "Dragomir Radev" },
    { "@type": "Person", "name": "Thomas Wolf" },
    { "@type": "Person", "name": "Verena Rieser" },
    { "@type": "Person", "name": "Sebastian Gehrmann" },
    { "@type": "Person", "name": "Yonatan Belinkov" },
    { "@type": "Person", "name": "Colin Raffel" },
    { "@type": "Person", "name": "Iz Beltagy" },
    { "@type": "Person", "name": "Sasha Luccioni" }
  ],
  "datePublished": "2022-11-09",
  "publisher": {
    "@type": "Organization",
    "name": "arXiv"
  },
  "identifier": {
    "@type": "PropertyValue",
    "propertyID": "DOI",
    "value": "10.48550/arXiv.2211.05100"
  },
  "url": "https://arxiv.org/abs/2211.05100",
  "description": "We introduce BLOOM, a 176-billion-parameter open-access multilingual large language model, trained on the ROOTS corpus covering 46 natural languages and 13 programming languages. BLOOM was developed through the BigScience project, a year-long open collaborative effort involving over 1,000 researchers from around the world. This paper describes the model architecture, training process, and ethical considerations, while emphasizing open science and transparent model release practices."
}
</script> <script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "EducationEvent",
  "name": "Machine Learning Methods in Detecting Alzheimer's Disease from Speech and Language",
  "startDate": "2019-11-16T09:00",
  "endDate": "2019-11-16T17:00",
  "location": {
    "@type": "Place",
    "name": "MLconf 2019",
    "address": {
      "@type": "PostalAddress",
      "addressLocality": "San Francisco",
      "addressRegion": "CA",
      "addressCountry": "US"
    }
  },
  "organizer": {
    "@type": "Organization",
    "name": "MLconf"
  },
  "performer": {
    "@type": "Person",
    "name": "Jekaterina Novikova",
    "givenName": "Jekaterina",
    "familyName": "Novikova",
    "alternateName": [
      "J. Novikova",
      "Jekaterina Novikova, PhD"
    ],
    "sameAs": [
      "https://scholar.google.ca/citations?user=C75JskwAAAAJ",
      "https://www.linkedin.com/in/jekaterina-novikova",
      "https://www.semanticscholar.org/author/Jekaterina-Novikova/2848048",
      "https://www.researchgate.net/profile/Jekaterina-Novikova-2",
      "https://x.com/J_Novikova_NLP",
      "https://bsky.app/profile/j-novikova-nlp.bsky.social",
      "https://orcid.org/0000-0003-4754-6126",
      "https://openreview.net/profile?id=~Jekaterina_Novikova1",
      "https://ieeexplore.ieee.org/author/37085694440",
      "https://huggingface.co/Jekaterina",
      "https://aclanthology.org/people/jekaterina-novikova/",
      "https://github.com/jnovikova",
      "https://loop.frontiersin.org/people/1157893/overview"
    ]
  },
  "description": "An invited talk at MLconf 2019 exploring machine learning methods in detecting Alzheimer's disease from speech and language.",
  "subjectOf": {
    "@type": "VideoObject",
    "name": "Detecting Cognitive Impairment Using AI - MLconf 2019",
    "description": "Recording of the invited talk given at MLconf 2019 in San Francisco.",
    "thumbnailUrl": "assets/img/mlconf2019.png",
    "uploadDate": "2019-11-17",
    "contentUrl": "https://youtu.be/MEWnP9SK1xY?si=1Sq2jCvPDy79OR9i"
  }
}
</script> <script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "EducationEvent",
  "name": "The Dual Nature of Consistency in Foundation Models: Challenges and Opportunities",
  "startDate": "2024-10-30T09:00",
  "endDate": "2024-10-30T17:00",
  "location": {
    "@type": "Place",
    "name": "TMLS 2024",
    "address": {
      "@type": "PostalAddress",
      "addressLocality": "Toronto",
      "addressRegion": "ON",
      "addressCountry": "CA"
    }
  },
  "performer": {
    "@type": "Person",
    "name": "Jekaterina Novikova",
    "givenName": "Jekaterina",
    "familyName": "Novikova",
    "alternateName": [
      "J. Novikova",
      "Jekaterina Novikova, PhD"
    ],
    "sameAs": [
      "https://scholar.google.ca/citations?user=C75JskwAAAAJ",
      "https://www.linkedin.com/in/jekaterina-novikova",
      "https://www.semanticscholar.org/author/Jekaterina-Novikova/2848048",
      "https://www.researchgate.net/profile/Jekaterina-Novikova-2",
      "https://x.com/J_Novikova_NLP",
      "https://bsky.app/profile/j-novikova-nlp.bsky.social",
      "https://orcid.org/0000-0003-4754-6126",
      "https://openreview.net/profile?id=~Jekaterina_Novikova1",
      "https://ieeexplore.ieee.org/author/37085694440",
      "https://huggingface.co/Jekaterina",
      "https://aclanthology.org/people/jekaterina-novikova/",
      "https://github.com/jnovikova",
      "https://loop.frontiersin.org/people/1157893/overview"
    ]
  },
  "description": "An invited talk at TMLS 2024 exploring consistency in LLMs and foundation models, how to measure it, what are the mitigation practices of its negative consequences, and what are the ways to use it to the advantage.",
  "subjectOf": {
    "@type": "VideoObject",
    "name": "The Dual Nature of Consistency in Foundation Models: Challenges and Opportunities - TMLS 2024",
    "description": "Recording of the invited talk given at TMLS 2024 in Toronto.",
    "thumbnailUrl": "assets/img/tmls2024.jpeg",
    "uploadDate": "2024-10-31",
    "contentUrl": "https://youtu.be/eQh3B7QBcEU?si=6CNUwDpT9ZYeVGX0"
  }
}
</script> <script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Event",
  "name": "Panel Discussion at NLP4Science Workshop ‚Äì EMNLP 2024",
  "startDate": "2024-11-16T09:00",
  "eventAttendanceMode": "https://schema.org/OnlineEventAttendanceMode",
  "eventStatus": "https://schema.org/EventScheduled",
  "location": {
    "@type": "VirtualLocation",
    "url": "https://sites.google.com/view/nlp4science/workshop-program?authuser=0#h.6ynhr2kyppdq"
  },
  "organizer": {
    "@type": "Organization",
    "name": "EMNLP 2024"
  },
  "subEvent": {
    "@type": "Event",
    "name": "NLP4Science Workshop",
    "startDate": "2024-11-16T09:00",
    "endDate": "2024-11-16T17:00",
    "location": {
      "@type": "Place",
      "name": "Miami",
      "address": {
        "@type": "PostalAddress",
        "addressLocality": "Miami",
        "addressRegion": "FL",
        "addressCountry": "US"
      }
    }
  },
  "performer": {
    "@type": "Person",
    "name": "Jekaterina Novikova",
    "givenName": "Jekaterina",
    "familyName": "Novikova",
    "alternateName": [
      "J. Novikova",
      "Jekaterina Novikova, PhD"
    ],
    "sameAs": [
      "https://scholar.google.ca/citations?user=C75JskwAAAAJ",
      "https://www.linkedin.com/in/jekaterina-novikova",
      "https://www.semanticscholar.org/author/Jekaterina-Novikova/2848048",
      "https://www.researchgate.net/profile/Jekaterina-Novikova-2",
      "https://x.com/J_Novikova_NLP",
      "https://bsky.app/profile/j-novikova-nlp.bsky.social",
      "https://orcid.org/0000-0003-4754-6126",
      "https://openreview.net/profile?id=~Jekaterina_Novikova1",
      "https://ieeexplore.ieee.org/author/37085694440",
      "https://huggingface.co/Jekaterina",
      "https://aclanthology.org/people/jekaterina-novikova/",
      "https://github.com/jnovikova",
      "https://loop.frontiersin.org/people/1157893/overview"
    ]
  },
  "description": "Panelist at the NLP4Science workshop during EMNLP 2024, discussing applications of NLP in scientific domains.",
  "url": "https://sites.google.com/view/nlp4science/workshop-program?authuser=0#h.6ynhr2kyppdq"
}
</script> <script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Event",
  "name": "Panel Discussion at ML4CMH Workshop ‚Äì AAAI 2024",
  "startDate": "2024-02-26T09:00",
  "eventAttendanceMode": "https://schema.org/OfflineEventAttendanceMode",
  "eventStatus": "https://schema.org/EventScheduled",
  "location": {
    "@type": "Place",
    "name": "Vancouver",
    "address": {
      "@type": "PostalAddress",
      "addressLocality": "Vancouver",
      "addressRegion": "BC",
      "addressCountry": "CA"
    }
  },
  "organizer": {
    "@type": "Organization",
    "name": "AAAI 2024"
  },
  "subEvent": {
    "@type": "Event",
    "name": "ML4CMH Workshop",
    "startDate": "2024-02-26T09:00",
    "endDate": "2024-02-26T17:00",
    "location": {
      "@type": "Place",
      "name": "Vancouver",
      "address": {
        "@type": "PostalAddress",
        "addressLocality": "Vancouver",
        "addressRegion": "BC",
        "addressCountry": "CA"
      }
    }
  },
  "performer": {
    "@type": "Person",
    "name": "Jekaterina Novikova",
    "givenName": "Jekaterina",
    "familyName": "Novikova",
    "alternateName": [
      "J. Novikova",
      "Jekaterina Novikova, PhD"
    ],
    "sameAs": [
      "https://scholar.google.ca/citations?user=C75JskwAAAAJ",
      "https://www.linkedin.com/in/jekaterina-novikova",
      "https://www.semanticscholar.org/author/Jekaterina-Novikova/2848048",
      "https://www.researchgate.net/profile/Jekaterina-Novikova-2",
      "https://x.com/J_Novikova_NLP",
      "https://bsky.app/profile/j-novikova-nlp.bsky.social",
      "https://orcid.org/0000-0003-4754-6126",
      "https://openreview.net/profile?id=~Jekaterina_Novikova1",
      "https://ieeexplore.ieee.org/author/37085694440",
      "https://huggingface.co/Jekaterina",
      "https://aclanthology.org/people/jekaterina-novikova/",
      "https://github.com/jnovikova",
      "https://loop.frontiersin.org/people/1157893/overview"
    ]
  },
  "description": "Panelist at the ML4CMH workshop during AAAI 2024, discussing applications of ML in cognitive and mental health.",
  "url": "https://winterlightlabs.github.io/ml4cmh2024/"
}
</script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.png?e347f6764308c5f308cdddc097b7371f"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jeknov.github.io/publications/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Jekaterina</span> Novikova </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Talks/Panels </a> </li> <li class="nav-item "> <a class="nav-link" href="/podcast/">WiAIR Podcast </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description">Select publications only. See my Google Scholar profile for a full list of publications.</p> </header> <article> <script src="/assets/js/bibsearch.js?1bc438ca9037884cc579601c09afd847" type="module"></script> <p><input type="text" id="bibsearch" spellcheck="false" autocomplete="off" class="search bibsearch-form-input" placeholder="Type to filter"></p> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">LLM</abbr> <figure> <picture> <img src="/assets/img/publication_preview/consistency.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="consistency.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="novikova2025consistency" class="col-sm-8"> <div class="title">Consistency in Language Models: Current Landscape, Challenges, and Future Directions</div> <div class="author"> <em>Jekaterina Novikova</em>, Carol Anderson, Borhane Blili-Hamelin, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Domenic Rosati, Subhabrata Majumdar' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>arXiv preprint arXiv:2505.00268</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2505.00268" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=C75JskwAAAAJ&amp;citation_for_view=C75JskwAAAAJ:t7zJ5fGR-2UC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>The hallmark of effective language use lies in consistency: expressing similar meanings in similar contexts and avoiding contradictions. While human communication naturally demonstrates this principle, state-of-the-art language models (LMs) struggle to maintain reliable consistency across task- and domain-specific applications. Here we examine the landscape of consistency research in LMs, analyze current approaches to measure aspects of consistency, and identify critical research gaps. Our findings point to an urgent need for quality benchmarks to measure and interdisciplinary approaches to ensure consistency while preserving utility.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">novikova2025consistency</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Consistency in Language Models: Current Landscape, Challenges, and Future Directions}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Novikova, Jekaterina and Anderson, Carol and Blili-Hamelin, Borhane and Rosati, Domenic and Majumdar, Subhabrata}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2505.00268}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">VLM</abbr> <figure> <picture> <img src="/assets/img/publication_preview/kaleidoscope.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="kaleidoscope.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="salazar2025kaleidoscopeinlanguageexamsmassively" class="col-sm-8"> <div class="title">Kaleidoscope: In-language Exams for Massively Multilingual Vision Evaluation</div> <div class="author"> Israfel Salazar, Manuel Fern√°ndez Burda, Shayekh Bin Islam, and <span class="more-authors" title="click to view 42 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '42 more authors' ? 'Arshia Soltani Moakhar, Shivalika Singh, Fabian Farestam, Angelika Romanou, Danylo Boiko, Dipika Khullar, Mike Zhang, Dominik Krzemi≈Ñski, Jekaterina Novikova, Lu√≠sa Shimabucoro, Joseph Marvin Imperial, Rishabh Maheshwary, Sharad Duwal, Alfonso Amayuelas, Swati Rajwal, Jebish Purbey, Ahmed Ruby, Nicholas Popoviƒç, Marek Suppa, Azmine Toushik Wasi, Ram Mohan Rao Kadiyala, Olga Tsymboi, Maksim Kostritsya, Bardia Soltani Moakhar, Gabriel Costa Merlin, Ot√°vio Ferracioli Coletti, Maral Jabbari Shiviari, MohammadAmin fard, Silvia Fernandez, Mar√≠a Grandury, Dmitry Abulkhanov, Drishti Sharma, Andre Guarnier De Mitri, Leticia Bossatto Marchezi, Setayesh Heydari, Johan Obando-Ceron, Nazar Kohut, Beyza Ermis, Desmond Elliott, Enzo Ferrante, Sara Hooker, Marzieh Fadaee' : '42 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">42 more authors</span> </div> <div class="periodical"> 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2504.07072" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/israfelsr/kaleidoscope" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://kaleidoscope-dataset.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=C75JskwAAAAJ&amp;citation_for_view=C75JskwAAAAJ:z_wVstp3MssC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>The evaluation of vision-language models (VLMs) has mainly relied on English-language benchmarks, leaving significant gaps in both multilingual and multicultural coverage. While multilingual benchmarks have expanded, both in size and languages, many rely on translations of English datasets, failing to capture cultural nuances. In this work, we propose Kaleidoscope, as the most comprehensive exam benchmark to date for the multilingual evaluation of vision-language models. Kaleidoscope is a large-scale, in-language multimodal benchmark designed to evaluate VLMs across diverse languages and visual inputs. Kaleidoscope covers 18 languages and 14 different subjects, amounting to a total of 20,911 multiple-choice questions. Built through an open science collaboration with a diverse group of researchers worldwide, Kaleidoscope ensures linguistic and cultural authenticity. We evaluate top-performing multilingual vision-language models and find that they perform poorly on low-resource languages and in complex multimodal scenarios. Our results highlight the need for progress on culturally inclusive multimodal evaluation frameworks.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">salazar2025kaleidoscopeinlanguageexamsmassively</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Kaleidoscope: In-language Exams for Massively Multilingual Vision Evaluation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Salazar, Israfel and Burda, Manuel Fern√°ndez and Islam, Shayekh Bin and Moakhar, Arshia Soltani and Singh, Shivalika and Farestam, Fabian and Romanou, Angelika and Boiko, Danylo and Khullar, Dipika and Zhang, Mike and Krzemi≈Ñski, Dominik and Novikova, Jekaterina and Shimabucoro, Lu√≠sa and Imperial, Joseph Marvin and Maheshwary, Rishabh and Duwal, Sharad and Amayuelas, Alfonso and Rajwal, Swati and Purbey, Jebish and Ruby, Ahmed and Popoviƒç, Nicholas and Suppa, Marek and Wasi, Azmine Toushik and Kadiyala, Ram Mohan Rao and Tsymboi, Olga and Kostritsya, Maksim and Moakhar, Bardia Soltani and da Costa Merlin, Gabriel and Coletti, Ot√°vio Ferracioli and Shiviari, Maral Jabbari and farahani fard, MohammadAmin and Fernandez, Silvia and Grandury, Mar√≠a and Abulkhanov, Dmitry and Sharma, Drishti and Mitri, Andre Guarnier De and Marchezi, Leticia Bossatto and Heydari, Setayesh and Obando-Ceron, Johan and Kohut, Nazar and Ermis, Beyza and Elliott, Desmond and Ferrante, Enzo and Hooker, Sara and Fadaee, Marzieh}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2504.07072}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CL}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2504.07072}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Eval</abbr> </div> <div id="liu2025human" class="col-sm-8"> <div class="title">Human-Centered Evaluation and Auditing of Language Models</div> <div class="author"> Yu Lu Liu, Wesley Hanwen Deng, Michelle S Lam, and <span class="more-authors" title="click to view 6 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '6 more authors' ? 'Motahhare Eslami, Juho Kim, Q Vera Liao, Wei Xu, Jekaterina Novikova, Ziang Xiao' : '6 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">6 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://doi.org/10.1145/3706599.3706729" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=C75JskwAAAAJ&amp;citation_for_view=C75JskwAAAAJ:XD-gHx7UXLsC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>The recent advancements in Large Language Models (LLMs) have significantly impacted numerous, and will impact more, real-world applications. However, these models also pose significant risks to individuals and society. To mitigate these issues and guide future model development, responsible evaluation and auditing of LLMs are essential. This workshop aims to address the current ‚Äúevaluation crisis‚Äù in LLM research and practice by bringing together HCI and AI researchers and practitioners to rethink LLM evaluation and auditing from a human-centered perspective. The workshop will explore topics around understanding stakeholders‚Äô needs and goals with evaluation and auditing LLMs, establishing human-centered evaluation and auditing methods, developing tools and resources to support these methods, building community and fostering collaboration. By soliciting papers, organizing invited keynote and panel, and facilitating group discussions, this workshop aims to develop a future research agenda for addressing the challenges in LLM evaluation and auditing. Following a successful first iteration of this workshop at CHI 2024, we introduce the theme of ‚Äúmind the context‚Äù for this second iteration, where participants will be encouraged to tackle the challenges and nuances of LLM evaluation and auditing in specific contexts.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">liu2025human</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Human-Centered Evaluation and Auditing of Language Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liu, Yu Lu and Deng, Wesley Hanwen and Lam, Michelle S and Eslami, Motahhare and Kim, Juho and Liao, Q Vera and Xu, Wei and Novikova, Jekaterina and Xiao, Ziang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--7}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3706599.3706729}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">LLM</abbr> <figure> <picture> <img src="/assets/img/publication_preview/include3.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="include3.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="romanou2025include" class="col-sm-8"> <div class="title">INCLUDE: Evaluating Multilingual Language Understanding with Regional Knowledge</div> <div class="author"> Angelika Romanou, Negar Foroutan, Anna Sotnikova, and <span class="more-authors" title="click to view 54 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '54 more authors' ? 'Sree Harsha Nelaturu, Shivalika Singh, Rishabh Maheshwary, Micol Altomare, Zeming Chen, Mohamed A. Haggag, Snegha A, Alfonso Amayuelas, Azril Hafizi Amirudin, Danylo Boiko, Michael Chang, Jenny Chim, Gal Cohen, Aditya Kumar Dalmia, Abraham Diress, Sharad Duwal, Daniil Dzenhaliou, Daniel Fernando Erazo Florez, Fabian Farestam, Joseph Marvin Imperial, Shayekh Bin Islam, Perttu Isotalo, Maral Jabbarishiviari, B√∂rje F. Karlsson, Eldar Khalilov, Christopher Klamm, Fajri Koto, Dominik Krzemi≈Ñski, Gabriel Adriano Melo, Syrielle Montariol, Yiyang Nan, Joel Niklaus, Jekaterina Novikova, Johan Samir Obando Ceron, Debjit Paul, Esther Ploeger, Jebish Purbey, Swati Rajwal, Selvan Sunitha Ravi, Sara Rydell, Roshan Santhosh, Drishti Sharma, Marjana Prifti Skenduli, Arshia Soltani Moakhar, Bardia moakhar, Ayush Kumar Tarun, Azmine Toushik Wasi, Thenuka Ovin Weerasinghe, Serhan Yilmaz, Mike Zhang, Imanol Schlag, Marzieh Fadaee, Sara Hooker, Antoine Bosselut' : '54 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">54 more authors</span> </div> <div class="periodical"> <em>In The Thirteenth International Conference on Learning Representations</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">üèÜ Awarded</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2411.19799" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://huggingface.co/datasets/CohereLabs/include-base-44" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=C75JskwAAAAJ&amp;citation_for_view=C75JskwAAAAJ:uJ-U7cs_P_0C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Spotlight at ICLR 2025</p> </div> <div class="abstract hidden"> <p>The performance differential of large language models (LLM) between languages hinders their effective deployment in many regions, inhibiting the potential economic and societal value of generative AI tools in many communities. However, the development of functional LLMs in many languages (\ie, multilingual LLMs) is bottlenecked by the lack of high-quality evaluation resources in languages other than English. Moreover, current practices in multilingual benchmark construction often translate English resources, ignoring the regional and cultural knowledge of the environments in which multilingual systems would be used. In this work, we construct an evaluation suite of 197,243 QA pairs from local exam sources to measure the capabilities of multilingual LLMs in a variety of regional contexts. Our novel resource, INCLUDE, is a comprehensive knowledge- and reasoning-centric benchmark across 44 written languages that evaluates multilingual LLMs for performance in the actual language environments where they would be deployed.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">romanou2025include</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{INCLUDE}: Evaluating Multilingual Language Understanding with Regional Knowledge}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Romanou, Angelika and Foroutan, Negar and Sotnikova, Anna and Nelaturu, Sree Harsha and Singh, Shivalika and Maheshwary, Rishabh and Altomare, Micol and Chen, Zeming and Haggag, Mohamed A. and A, Snegha and Amayuelas, Alfonso and Amirudin, Azril Hafizi and Boiko, Danylo and Chang, Michael and Chim, Jenny and Cohen, Gal and Dalmia, Aditya Kumar and Diress, Abraham and Duwal, Sharad and Dzenhaliou, Daniil and Florez, Daniel Fernando Erazo and Farestam, Fabian and Imperial, Joseph Marvin and Islam, Shayekh Bin and Isotalo, Perttu and Jabbarishiviari, Maral and Karlsson, B{\"o}rje F. and Khalilov, Eldar and Klamm, Christopher and Koto, Fajri and Krzemi{\'n}ski, Dominik and de Melo, Gabriel Adriano and Montariol, Syrielle and Nan, Yiyang and Niklaus, Joel and Novikova, Jekaterina and Ceron, Johan Samir Obando and Paul, Debjit and Ploeger, Esther and Purbey, Jebish and Rajwal, Swati and Ravi, Selvan Sunitha and Rydell, Sara and Santhosh, Roshan and Sharma, Drishti and Skenduli, Marjana Prifti and Moakhar, Arshia Soltani and soltani moakhar, Bardia and Tarun, Ayush Kumar and Wasi, Azmine Toushik and Weerasinghe, Thenuka Ovin and Yilmaz, Serhan and Zhang, Mike and Schlag, Imanol and Fadaee, Marzieh and Hooker, Sara and Bosselut, Antoine}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The Thirteenth International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=k3gCieTXeY}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">LLM</abbr> <figure> <picture> <img src="/assets/img/publication_preview/bloom.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="bloom.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="workshop2023bloom176bparameteropenaccessmultilingual" class="col-sm-8"> <div class="title">BLOOM: A 176B-Parameter Open-Access Multilingual Language Model</div> <div class="author"> BigScience Workshop, :, Teven Le Scao, and <span class="more-authors" title="click to view 391 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '391 more authors' ? 'Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Iliƒá, Daniel Hesslow, Roman Castagn√©, Alexandra Sasha Luccioni, Fran√ßois Yvon, Matthias Gall√©, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Beno√Æt Sagot, Niklas Muennighoff, Albert Villanova Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Lauren√ßon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel Strien, David Ifeoluwa Adelani, Dragomir Radev, Eduardo Gonz√°lez Ponferrada, Efrat Levkovizh, Ethan Kim, Eyal Bar Natan, Francesco De Toni, G√©rard Dupont, Germ√°n Kruszewski, Giada Pistilli, Hady Elsahar, Hamza Benyamina, Hieu Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, J√∂rg Frohberg, Joseph Tobing, Joydeep Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro Von Werra, Leon Weber, Long Phan, Loubna Ben allal, Ludovic Tanguy, Manan Dey, Manuel Romero Mu√±oz, Maraim Masoud, Mar√≠a Grandury, Mario ≈†a≈°ko, Max Huang, Maximin Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh Chien Vu, Mohammad A. Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espejel, Ona Gibert, Paulo Villegas, Peter Henderson, Pierre Colombo, Priscilla Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto Luis L√≥pez, Rui Ribeiro, Salomey Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose, Shamsuddeen Hassan Muhammad, Shanya Sharma, Shayne Longpre, Somaieh Nikpoor, Stanislav Silberberg, Suhas Pai, Sydney Zink, Tiago Timponi Torrent, Timo Schick, Tristan Thrush, Valentin Danchev, Vassilina Nikoulina, Veronika Laippala, Violette Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Davut Emre Ta≈üar, Elizabeth Salesky, Sabrina J. Mielke, Wilson Y. Lee, Abheesht Sharma, Andrea Santilli, Antoine Chaffin, Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey, Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M Saiful Bari, Maged S. Al-shaibani, Matteo Manica, Nihal Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen H. Bach, Taewoon Kim, Tali Bers, Thibault Fevry, Trishala Neeraj, Urmish Thakker, Vikas Raunak, Xiangru Tang, Zheng-Xin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri, Hadar Tojarieh, Adam Roberts, Hyung Won Chung, Jaesung Tae, Jason Phang, Ofir Press, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar Sanseviero, Patrick Platen, Pierre Cornette, Pierre Fran√ßois Lavall√©e, R√©mi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, St√©phane Requena, Suraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun Subramonian, Aur√©lie N√©v√©ol, Charles Lovering, Dan Garrette, Deepak Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, Ekaterina Voloshina, Eli Bogdanov, Genta Indra Winata, Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina Novikova, Jessica Zosa Forde, Jordan Clive, Jungo Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton Cheng, Oleg Serikov, Omer Antverg, Oskar Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, Shachar Mirkin, Shani Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov, Vladislav Mikhailov, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bamberger, Zdenƒõk Kasner, Alice Rueda, Amanda Pestana, Amir Feizpour, Ammar Khan, Amy Faranak, Ana Santos, Anthony Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi, Benjamin Ajibade, Bharat Saxena, Carlos Mu√±oz Ferrandis, Daniel McDuff, Danish Contractor, David Lansky, Davis David, Douwe Kiela, Duong A. Nguyen, Edward Tan, Emi Baylor, Ezinwanne Ozoani, Fatima Mirza, Frankline Ononiwu, Habib Rezanejad, Hessie Jones, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Nejadgholi, Jesse Passmore, Josh Seltzer, Julio Bonis Sanz, Livia Dutra, Mairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna, Mike Qiu, Muhammed Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel, Ran An, Rasmus Kromann, Ryan Hao, Samira Alizadeh, Sarmad Shubber, Silas Wang, Sourav Roy, Sylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le, Yoyo Yang, Zach Nguyen, Abhinav Ramesh Kashyap, Alfredo Palasciano, Alison Callahan, Anima Shukla, Antonio Miranda-Escalada, Ayush Singh, Benjamin Beilharz, Bo Wang, Caio Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu, Cl√©mentine Fourrier, Daniel Le√≥n Peri√±√°n, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully Burns, Helena U. Vrabec, Imane Bello, Ishani Dash, Jihyun Kang, John Giorgi, Jonas Golde, Jose David Posada, Karthik Rangasai Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa Shinzato, Madeleine Hahn Bykhovetz, Maiko Takeuchi, Marc P√†mies, Maria A Castillo, Marianna Nezhurina, Mario S√§nger, Matthias Samwald, Michael Cullan, Michael Weinberg, Michiel De Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan Dahlberg, Nicholas Michio Broad, Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya Chandrasekhar, Renata Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter, Sushil Bharati, Tanmay Laud, Th√©o Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis Labrak, Yash Shailesh Bajaj, Yash Venkatraman, Yifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, Thomas Wolf' : '391 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">391 more authors</span> </div> <div class="periodical"> 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2211.05100" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://huggingface.co/bigscience/bloom" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=C75JskwAAAAJ&amp;citation_for_view=C75JskwAAAAJ:NJ774b8OgUMC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">workshop2023bloom176bparameteropenaccessmultilingual</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{BLOOM: A 176B-Parameter Open-Access Multilingual Language Model}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Workshop, BigScience and : and Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Iliƒá, Suzana and Hesslow, Daniel and Castagn√©, Roman and Luccioni, Alexandra Sasha and Yvon, Fran√ßois and Gall√©, Matthias and Tow, Jonathan and Rush, Alexander M. and Biderman, Stella and Webson, Albert and Ammanamanchi, Pawan Sasanka and Wang, Thomas and Sagot, Beno√Æt and Muennighoff, Niklas and del Moral, Albert Villanova and Ruwase, Olatunji and Bawden, Rachel and Bekman, Stas and McMillan-Major, Angelina and Beltagy, Iz and Nguyen, Huu and Saulnier, Lucile and Tan, Samson and Suarez, Pedro Ortiz and Sanh, Victor and Lauren√ßon, Hugo and Jernite, Yacine and Launay, Julien and Mitchell, Margaret and Raffel, Colin and Gokaslan, Aaron and Simhi, Adi and Soroa, Aitor and Aji, Alham Fikri and Alfassy, Amit and Rogers, Anna and Nitzav, Ariel Kreisberg and Xu, Canwen and Mou, Chenghao and Emezue, Chris and Klamm, Christopher and Leong, Colin and van Strien, Daniel and Adelani, David Ifeoluwa and Radev, Dragomir and Ponferrada, Eduardo Gonz√°lez and Levkovizh, Efrat and Kim, Ethan and Natan, Eyal Bar and Toni, Francesco De and Dupont, G√©rard and Kruszewski, Germ√°n and Pistilli, Giada and Elsahar, Hady and Benyamina, Hamza and Tran, Hieu and Yu, Ian and Abdulmumin, Idris and Johnson, Isaac and Gonzalez-Dios, Itziar and de la Rosa, Javier and Chim, Jenny and Dodge, Jesse and Zhu, Jian and Chang, Jonathan and Frohberg, J√∂rg and Tobing, Joseph and Bhattacharjee, Joydeep and Almubarak, Khalid and Chen, Kimbo and Lo, Kyle and Werra, Leandro Von and Weber, Leon and Phan, Long and allal, Loubna Ben and Tanguy, Ludovic and Dey, Manan and Mu√±oz, Manuel Romero and Masoud, Maraim and Grandury, Mar√≠a and ≈†a≈°ko, Mario and Huang, Max and Coavoux, Maximin and Singh, Mayank and Jiang, Mike Tian-Jian and Vu, Minh Chien and Jauhar, Mohammad A. and Ghaleb, Mustafa and Subramani, Nishant and Kassner, Nora and Khamis, Nurulaqilla and Nguyen, Olivier and Espejel, Omar and de Gibert, Ona and Villegas, Paulo and Henderson, Peter and Colombo, Pierre and Amuok, Priscilla and Lhoest, Quentin and Harliman, Rheza and Bommasani, Rishi and L√≥pez, Roberto Luis and Ribeiro, Rui and Osei, Salomey and Pyysalo, Sampo and Nagel, Sebastian and Bose, Shamik and Muhammad, Shamsuddeen Hassan and Sharma, Shanya and Longpre, Shayne and Nikpoor, Somaieh and Silberberg, Stanislav and Pai, Suhas and Zink, Sydney and Torrent, Tiago Timponi and Schick, Timo and Thrush, Tristan and Danchev, Valentin and Nikoulina, Vassilina and Laippala, Veronika and Lepercq, Violette and Prabhu, Vrinda and Alyafeai, Zaid and Talat, Zeerak and Raja, Arun and Heinzerling, Benjamin and Si, Chenglei and Ta≈üar, Davut Emre and Salesky, Elizabeth and Mielke, Sabrina J. and Lee, Wilson Y. and Sharma, Abheesht and Santilli, Andrea and Chaffin, Antoine and Stiegler, Arnaud and Datta, Debajyoti and Szczechla, Eliza and Chhablani, Gunjan and Wang, Han and Pandey, Harshit and Strobelt, Hendrik and Fries, Jason Alan and Rozen, Jos and Gao, Leo and Sutawika, Lintang and Bari, M Saiful and Al-shaibani, Maged S. and Manica, Matteo and Nayak, Nihal and Teehan, Ryan and Albanie, Samuel and Shen, Sheng and Ben-David, Srulik and Bach, Stephen H. and Kim, Taewoon and Bers, Tali and Fevry, Thibault and Neeraj, Trishala and Thakker, Urmish and Raunak, Vikas and Tang, Xiangru and Yong, Zheng-Xin and Sun, Zhiqing and Brody, Shaked and Uri, Yallow and Tojarieh, Hadar and Roberts, Adam and Chung, Hyung Won and Tae, Jaesung and Phang, Jason and Press, Ofir and Li, Conglong and Narayanan, Deepak and Bourfoune, Hatim and Casper, Jared and Rasley, Jeff and Ryabinin, Max and Mishra, Mayank and Zhang, Minjia and Shoeybi, Mohammad and Peyrounette, Myriam and Patry, Nicolas and Tazi, Nouamane and Sanseviero, Omar and von Platen, Patrick and Cornette, Pierre and Lavall√©e, Pierre Fran√ßois and Lacroix, R√©mi and Rajbhandari, Samyam and Gandhi, Sanchit and Smith, Shaden and Requena, St√©phane and Patil, Suraj and Dettmers, Tim and Baruwa, Ahmed and Singh, Amanpreet and Cheveleva, Anastasia and Ligozat, Anne-Laure and Subramonian, Arjun and N√©v√©ol, Aur√©lie and Lovering, Charles and Garrette, Dan and Tunuguntla, Deepak and Reiter, Ehud and Taktasheva, Ekaterina and Voloshina, Ekaterina and Bogdanov, Eli and Winata, Genta Indra and Schoelkopf, Hailey and Kalo, Jan-Christoph and Novikova, Jekaterina and Forde, Jessica Zosa and Clive, Jordan and Kasai, Jungo and Kawamura, Ken and Hazan, Liam and Carpuat, Marine and Clinciu, Miruna and Kim, Najoung and Cheng, Newton and Serikov, Oleg and Antverg, Omer and van der Wal, Oskar and Zhang, Rui and Zhang, Ruochen and Gehrmann, Sebastian and Mirkin, Shachar and Pais, Shani and Shavrina, Tatiana and Scialom, Thomas and Yun, Tian and Limisiewicz, Tomasz and Rieser, Verena and Protasov, Vitaly and Mikhailov, Vladislav and Pruksachatkun, Yada and Belinkov, Yonatan and Bamberger, Zachary and Kasner, Zdenƒõk and Rueda, Alice and Pestana, Amanda and Feizpour, Amir and Khan, Ammar and Faranak, Amy and Santos, Ana and Hevia, Anthony and Unldreaj, Antigona and Aghagol, Arash and Abdollahi, Arezoo and Tammour, Aycha and HajiHosseini, Azadeh and Behroozi, Bahareh and Ajibade, Benjamin and Saxena, Bharat and Ferrandis, Carlos Mu√±oz and McDuff, Daniel and Contractor, Danish and Lansky, David and David, Davis and Kiela, Douwe and Nguyen, Duong A. and Tan, Edward and Baylor, Emi and Ozoani, Ezinwanne and Mirza, Fatima and Ononiwu, Frankline and Rezanejad, Habib and Jones, Hessie and Bhattacharya, Indrani and Solaiman, Irene and Sedenko, Irina and Nejadgholi, Isar and Passmore, Jesse and Seltzer, Josh and Sanz, Julio Bonis and Dutra, Livia and Samagaio, Mairon and Elbadri, Maraim and Mieskes, Margot and Gerchick, Marissa and Akinlolu, Martha and McKenna, Michael and Qiu, Mike and Ghauri, Muhammed and Burynok, Mykola and Abrar, Nafis and Rajani, Nazneen and Elkott, Nour and Fahmy, Nour and Samuel, Olanrewaju and An, Ran and Kromann, Rasmus and Hao, Ryan and Alizadeh, Samira and Shubber, Sarmad and Wang, Silas and Roy, Sourav and Viguier, Sylvain and Le, Thanh and Oyebade, Tobi and Le, Trieu and Yang, Yoyo and Nguyen, Zach and Kashyap, Abhinav Ramesh and Palasciano, Alfredo and Callahan, Alison and Shukla, Anima and Miranda-Escalada, Antonio and Singh, Ayush and Beilharz, Benjamin and Wang, Bo and Brito, Caio and Zhou, Chenxi and Jain, Chirag and Xu, Chuxin and Fourrier, Cl√©mentine and Peri√±√°n, Daniel Le√≥n and Molano, Daniel and Yu, Dian and Manjavacas, Enrique and Barth, Fabio and Fuhrimann, Florian and Altay, Gabriel and Bayrak, Giyaseddin and Burns, Gully and Vrabec, Helena U. and Bello, Imane and Dash, Ishani and Kang, Jihyun and Giorgi, John and Golde, Jonas and Posada, Jose David and Sivaraman, Karthik Rangasai and Bulchandani, Lokesh and Liu, Lu and Shinzato, Luisa and de Bykhovetz, Madeleine Hahn and Takeuchi, Maiko and P√†mies, Marc and Castillo, Maria A and Nezhurina, Marianna and S√§nger, Mario and Samwald, Matthias and Cullan, Michael and Weinberg, Michael and Wolf, Michiel De and Mihaljcic, Mina and Liu, Minna and Freidank, Moritz and Kang, Myungsun and Seelam, Natasha and Dahlberg, Nathan and Broad, Nicholas Michio and Muellner, Nikolaus and Fung, Pascale and Haller, Patrick and Chandrasekhar, Ramya and Eisenberg, Renata and Martin, Robert and Canalli, Rodrigo and Su, Rosaline and Su, Ruisi and Cahyawijaya, Samuel and Garda, Samuele and Deshmukh, Shlok S and Mishra, Shubhanshu and Kiblawi, Sid and Ott, Simon and Sang-aroonsiri, Sinee and Kumar, Srishti and Schweter, Stefan and Bharati, Sushil and Laud, Tanmay and Gigant, Th√©o and Kainuma, Tomoya and Kusa, Wojciech and Labrak, Yanis and Bajaj, Yash Shailesh and Venkatraman, Yash and Xu, Yifan and Xu, Yingxin and Xu, Yu and Tan, Zhe and Xie, Zhongli and Ye, Zifan and Bras, Mathilde and Belkada, Younes and Wolf, Thomas}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2211.05100}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CL}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2211.05100}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Eval</abbr> <figure> <picture> <img src="/assets/img/publication_preview/bigbench.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="bigbench.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="srivastava2023beyond" class="col-sm-8"> <div class="title">Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models</div> <div class="author"> Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, and <span class="more-authors" title="click to view 447 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '447 more authors' ? 'Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adri√† Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman S. Iyer, Anders Johan Andreassen, Andrea Madotto, Andrea Santilli, Andreas Stuhlm√ºller, Andrew M. Dai, Andrew La, Andrew Kyle Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karaka≈ü, B. Ryan Roberts, Bao Sheng Loe, Barret Zoph, Bart≈Çomiej Bojanowski, Batuhan √ñzyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno Stein, Berk Ekmekci, Bill Yuchen Lin, Blake Howald, Bryan Orinion, Cameron Diao, Cameron Dour, Catherine Stinson, Cedrick Argueta, Cesar Ferri, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu, Chris Callison-Burch, Christopher Waites, Christian Voigt, Christopher D Manning, Christopher Potts, Cindy Ramirez, Clara E. Rivera, Clemencia Siro, Colin Raffel, Courtney Ashcraft, Cristina Garbacea, Damien Sileo, Dan Garrette, Dan Hendrycks, Dan Kilman, Dan Roth, C. Daniel Freeman, Daniel Khashabi, Daniel Levy, Daniel Moseguƒ±ÃÅ Gonz√°lez, Danielle Perszyk, Danny Hernandez, Danqi Chen, Daphne Ippolito, Dar Gilboa, David Dohan, David Drakard, David Jurgens, Debajyoti Datta, Deep Ganguli, Denis Emelin, Denis Kleyko, Deniz Yuret, Derek Chen, Derek Tam, Dieuwke Hupkes, Diganta Misra, Dilyar Buzan, Dimitri Coelho Mollo, Diyi Yang, Dong-Ho Lee, Dylan Schrader, Ekaterina Shutova, Ekin Dogus Cubuk, Elad Segal, Eleanor Hagerman, Elizabeth Barnes, Elizabeth Donoway, Ellie Pavlick, Emanuele Rodol√†, Emma Lam, Eric Chu, Eric Tang, Erkut Erdem, Ernie Chang, Ethan A Chi, Ethan Dyer, Ethan Jerzak, Ethan Kim, Eunice Engefu Manyasi, Evgenii Zheltonozhskii, Fanyue Xia, Fatemeh Siar, Fernando Martƒ±ÃÅnez-Plumed, Francesca Happ√©, Francois Chollet, Frieda Rong, Gaurav Mishra, Genta Indra Winata, Gerard Melo, Germ√†n Kruszewski, Giambattista Parascandolo, Giorgio Mariani, Gloria Xinyue Wang, Gonzalo Jaimovitch-Lopez, Gregor Betz, Guy Gur-Ari, Hana Galijasevic, Hannah Kim, Hannah Rashkin, Hannaneh Hajishirzi, Harsh Mehta, Hayden Bogar, Henry Francis Anthony Shevlin, Hinrich Schuetze, Hiromu Yakura, Hongming Zhang, Hugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet, Jack Geissinger, Jackson Kernion, Jacob Hilton, Jaehoon Lee, Jaime Fern√°ndez Fisac, James B Simon, James Koppel, James Zheng, James Zou, Jan Kocon, Jana Thompson, Janelle Wingfield, Jared Kaplan, Jarema Radom, Jascha Sohl-Dickstein, Jason Phang, Jason Wei, Jason Yosinski, Jekaterina Novikova, Jelle Bosscher, Jennifer Marsh, Jeremy Kim, Jeroen Taal, Jesse Engel, Jesujoba Alabi, Jiacheng Xu, Jiaming Song, Jillian Tang, Joan Waweru, John Burden, John Miller, John U. Balis, Jonathan Batchelder, Jonathan Berant, J√∂rg Frohberg, Jos Rozen, Jose Hernandez-Orallo, Joseph Boudeman, Joseph Guerr, Joseph Jones, Joshua B. Tenenbaum, Joshua S. Rule, Joyce Chua, Kamil Kanclerz, Karen Livescu, Karl Krauth, Karthik Gopalakrishnan, Katerina Ignatyeva, Katja Markert, Kaustubh Dhole, Kevin Gimpel, Kevin Omondi, Kory Wallace Mathewson, Kristen Chiafullo, Ksenia Shkaruta, Kumar Shridhar, Kyle McDonell, Kyle Richardson, Laria Reynolds, Leo Gao, Li Zhang, Liam Dugan, Lianhui Qin, Lidia Contreras-Ochando, Louis-Philippe Morency, Luca Moschella, Lucas Lam, Lucy Noble, Ludwig Schmidt, Luheng He, Luis Oliveros-Col√≥n, Luke Metz, L√ºtfi Kerem Senel, Maarten Bosma, Maarten Sap, Maartje Ter Hoeve, Maheen Farooqi, Manaal Faruqui, Mantas Mazeika, Marco Baturan, Marco Marelli, Marco Maru, Maria Jose Ramirez-Quintana, Marie Tolkiehn, Mario Giulianelli, Martha Lewis, Martin Potthast, Matthew L Leavitt, Matthias Hagen, M√°ty√°s Schubert, Medina Orduna Baitemirova, Melody Arnaud, Melvin McElrath, Michael Andrew Yee, Michael Cohen, Michael Gu, Michael Ivanitskiy, Michael Starritt, Michael Strube, Micha≈Ç Swƒôdrowski, Michele Bevilacqua, Michihiro Yasunaga, Mihir Kale, Mike Cain, Mimee Xu, Mirac Suzgun, Mitch Walker, Mo Tiwari, Mohit Bansal, Moin Aminnaseri, Mor Geva, Mozhdeh Gheini, Mukund Varma T, Nanyun Peng, Nathan Andrew Chi, Nayeon Lee, Neta Gur-Ari Krakover, Nicholas Cameron, Nicholas Roberts, Nick Doiron, Nicole Martinez, Nikita Nangia, Niklas Deckers, Niklas Muennighoff, Nitish Shirish Keskar, Niveditha S. Iyer, Noah Constant, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar Agha, Omar Elbaghdadi, Omer Levy, Owain Evans, Pablo Antonio Moreno Casares, Parth Doshi, Pascale Fung, Paul Pu Liang, Paul Vicol, Pegah Alipoormolabashi, Peiyuan Liao, Percy Liang, Peter W Chang, Peter Eckersley, Phu Mon Htut, Pinyu Hwang, Piotr Mi≈Çkowski, Piyush Patil, Pouya Pezeshkpour, Priti Oli, Qiaozhu Mei, Qing Lyu, Qinlang Chen, Rabin Banjade, Rachel Etta Rudolph, Raefer Gabriel, Rahel Habacker, Ramon Risco, Rapha√´l Milli√®re, Rhythm Garg, Richard Barnes, Rif A. Saurous, Riku Arakawa, Robbe Raymaekers, Robert Frank, Rohan Sikand, Roman Novak, Roman Sitelew, Ronan Le Bras, Rosanne Liu, Rowan Jacobs, Rui Zhang, Russ Salakhutdinov, Ryan Andrew Chi, Seungjae Ryan Lee, Ryan Stovall, Ryan Teehan, Rylan Yang, Sahib Singh, Saif M. Mohammad, Sajant Anand, Sam Dillavou, Sam Shleifer, Sam Wiseman, Samuel Gruetter, Samuel R. Bowman, Samuel Stern Schoenholz, Sanghyun Han, Sanjeev Kwatra, Sarah A. Rous, Sarik Ghazarian, Sayan Ghosh, Sean Casey, Sebastian Bischoff, Sebastian Gehrmann, Sebastian Schuster, Sepideh Sadeghi, Shadi Hamdan, Sharon Zhou, Shashank Srivastava, Sherry Shi, Shikhar Singh, Shima Asaadi, Shixiang Shane Gu, Shubh Pachchigar, Shubham Toshniwal, Shyam Upadhyay, Shyamolima Shammie Debnath, Siamak Shakeri, Simon Thormeyer, Simone Melzi, Siva Reddy, Sneha Priscilla Makini, Soo-Hwan Lee, Spencer Torene, Sriharsha Hatwar, Stanislas Dehaene, Stefan Divic, Stefano Ermon, Stella Biderman, Stephanie Lin, Stephen Prasad, Steven Piantadosi, Stuart Shieber, Summer Misherghi, Svetlana Kiritchenko, Swaroop Mishra, Tal Linzen, Tal Schuster, Tao Li, Tao Yu, Tariq Ali, Tatsunori Hashimoto, Te-Lin Wu, Th√©o Desbordes, Theodore Rothschild, Thomas Phan, Tianle Wang, Tiberius Nkinyili, Timo Schick, Timofei Kornev, Titus Tunduny, Tobias Gerstenberg, Trenton Chang, Trishala Neeraj, Tushar Khot, Tyler Shultz, Uri Shaham, Vedant Misra, Vera Demberg, Victoria Nyamai, Vikas Raunak, Vinay Venkatesh Ramasesh, prabhu, Vishakh Padmakumar, Vivek Srikumar, William Fedus, William Saunders, William Zhang, Wout Vossen, Xiang Ren, Xiaoyu Tong, Xinran Zhao, Xinyi Wu, Xudong Shen, Yadollah Yaghoobzadeh, Yair Lakretz, Yangqiu Song, Yasaman Bahri, Yejin Choi, Yichi Yang, Sophie Hao, Yifu Chen, Yonatan Belinkov, Yu Hou, Yufang Hou, Yuntao Bai, Zachary Seid, Zhuoye Zhao, Zijian Wang, Zijie J. Wang, Zirui Wang, Ziyi Wu' : '447 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">447 more authors</span> </div> <div class="periodical"> <em>Transactions on Machine Learning Research</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">üèÜ Awarded</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2206.04615" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/google/BIG-bench" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=C75JskwAAAAJ&amp;citation_for_view=C75JskwAAAAJ:PR6Y55bgFSsC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Featured Certification by the TMLR Outstanding Paper Committee</p> </div> <div class="abstract hidden"> <p>Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI‚Äôs GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit ‚Äôbreakthrough‚Äô behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">srivastava2023beyond</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R. and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\`a} and Kluska, Agnieszka and Lewkowycz, Aitor and Agarwal, Akshat and Power, Alethea and Ray, Alex and Warstadt, Alex and Kocurek, Alexander W. and Safaya, Ali and Tazarv, Ali and Xiang, Alice and Parrish, Alicia and Nie, Allen and Hussain, Aman and Askell, Amanda and Dsouza, Amanda and Slone, Ambrose and Rahane, Ameet and Iyer, Anantharaman S. and Andreassen, Anders Johan and Madotto, Andrea and Santilli, Andrea and Stuhlm{\"u}ller, Andreas and Dai, Andrew M. and La, Andrew and Lampinen, Andrew Kyle and Zou, Andy and Jiang, Angela and Chen, Angelica and Vuong, Anh and Gupta, Animesh and Gottardi, Anna and Norelli, Antonio and Venkatesh, Anu and Gholamidavoodi, Arash and Tabassum, Arfa and Menezes, Arul and Kirubarajan, Arun and Mullokandov, Asher and Sabharwal, Ashish and Herrick, Austin and Efrat, Avia and Erdem, Aykut and Karaka{\c{s}}, Ayla and Roberts, B. Ryan and Loe, Bao Sheng and Zoph, Barret and Bojanowski, Bart{\l}omiej and {\"O}zyurt, Batuhan and Hedayatnia, Behnam and Neyshabur, Behnam and Inden, Benjamin and Stein, Benno and Ekmekci, Berk and Lin, Bill Yuchen and Howald, Blake and Orinion, Bryan and Diao, Cameron and Dour, Cameron and Stinson, Catherine and Argueta, Cedrick and Ferri, Cesar and Singh, Chandan and Rathkopf, Charles and Meng, Chenlin and Baral, Chitta and Wu, Chiyu and Callison-Burch, Chris and Waites, Christopher and Voigt, Christian and Manning, Christopher D and Potts, Christopher and Ramirez, Cindy and Rivera, Clara E. and Siro, Clemencia and Raffel, Colin and Ashcraft, Courtney and Garbacea, Cristina and Sileo, Damien and Garrette, Dan and Hendrycks, Dan and Kilman, Dan and Roth, Dan and Freeman, C. Daniel and Khashabi, Daniel and Levy, Daniel and Gonz{\'a}lez, Daniel Mosegu{\'\i} and Perszyk, Danielle and Hernandez, Danny and Chen, Danqi and Ippolito, Daphne and Gilboa, Dar and Dohan, David and Drakard, David and Jurgens, David and Datta, Debajyoti and Ganguli, Deep and Emelin, Denis and Kleyko, Denis and Yuret, Deniz and Chen, Derek and Tam, Derek and Hupkes, Dieuwke and Misra, Diganta and Buzan, Dilyar and Mollo, Dimitri Coelho and Yang, Diyi and Lee, Dong-Ho and Schrader, Dylan and Shutova, Ekaterina and Cubuk, Ekin Dogus and Segal, Elad and Hagerman, Eleanor and Barnes, Elizabeth and Donoway, Elizabeth and Pavlick, Ellie and Rodol{\`a}, Emanuele and Lam, Emma and Chu, Eric and Tang, Eric and Erdem, Erkut and Chang, Ernie and Chi, Ethan A and Dyer, Ethan and Jerzak, Ethan and Kim, Ethan and Manyasi, Eunice Engefu and Zheltonozhskii, Evgenii and Xia, Fanyue and Siar, Fatemeh and Mart{\'\i}nez-Plumed, Fernando and Happ{\'e}, Francesca and Chollet, Francois and Rong, Frieda and Mishra, Gaurav and Winata, Genta Indra and de Melo, Gerard and Kruszewski, Germ{\`a}n and Parascandolo, Giambattista and Mariani, Giorgio and Wang, Gloria Xinyue and Jaimovitch-Lopez, Gonzalo and Betz, Gregor and Gur-Ari, Guy and Galijasevic, Hana and Kim, Hannah and Rashkin, Hannah and Hajishirzi, Hannaneh and Mehta, Harsh and Bogar, Hayden and Shevlin, Henry Francis Anthony and Schuetze, Hinrich and Yakura, Hiromu and Zhang, Hongming and Wong, Hugh Mee and Ng, Ian and Noble, Isaac and Jumelet, Jaap and Geissinger, Jack and Kernion, Jackson and Hilton, Jacob and Lee, Jaehoon and Fisac, Jaime Fern{\'a}ndez and Simon, James B and Koppel, James and Zheng, James and Zou, James and Kocon, Jan and Thompson, Jana and Wingfield, Janelle and Kaplan, Jared and Radom, Jarema and Sohl-Dickstein, Jascha and Phang, Jason and Wei, Jason and Yosinski, Jason and Novikova, Jekaterina and Bosscher, Jelle and Marsh, Jennifer and Kim, Jeremy and Taal, Jeroen and Engel, Jesse and Alabi, Jesujoba and Xu, Jiacheng and Song, Jiaming and Tang, Jillian and Waweru, Joan and Burden, John and Miller, John and Balis, John U. and Batchelder, Jonathan and Berant, Jonathan and Frohberg, J{\"o}rg and Rozen, Jos and Hernandez-Orallo, Jose and Boudeman, Joseph and Guerr, Joseph and Jones, Joseph and Tenenbaum, Joshua B. and Rule, Joshua S. and Chua, Joyce and Kanclerz, Kamil and Livescu, Karen and Krauth, Karl and Gopalakrishnan, Karthik and Ignatyeva, Katerina and Markert, Katja and Dhole, Kaustubh and Gimpel, Kevin and Omondi, Kevin and Mathewson, Kory Wallace and Chiafullo, Kristen and Shkaruta, Ksenia and Shridhar, Kumar and McDonell, Kyle and Richardson, Kyle and Reynolds, Laria and Gao, Leo and Zhang, Li and Dugan, Liam and Qin, Lianhui and Contreras-Ochando, Lidia and Morency, Louis-Philippe and Moschella, Luca and Lam, Lucas and Noble, Lucy and Schmidt, Ludwig and He, Luheng and Oliveros-Col{\'o}n, Luis and Metz, Luke and Senel, L{\"u}tfi Kerem and Bosma, Maarten and Sap, Maarten and Hoeve, Maartje Ter and Farooqi, Maheen and Faruqui, Manaal and Mazeika, Mantas and Baturan, Marco and Marelli, Marco and Maru, Marco and Ramirez-Quintana, Maria Jose and Tolkiehn, Marie and Giulianelli, Mario and Lewis, Martha and Potthast, Martin and Leavitt, Matthew L and Hagen, Matthias and Schubert, M{\'a}ty{\'a}s and Baitemirova, Medina Orduna and Arnaud, Melody and McElrath, Melvin and Yee, Michael Andrew and Cohen, Michael and Gu, Michael and Ivanitskiy, Michael and Starritt, Michael and Strube, Michael and Sw{\k{e}}drowski, Micha{\l} and Bevilacqua, Michele and Yasunaga, Michihiro and Kale, Mihir and Cain, Mike and Xu, Mimee and Suzgun, Mirac and Walker, Mitch and Tiwari, Mo and Bansal, Mohit and Aminnaseri, Moin and Geva, Mor and Gheini, Mozhdeh and T, Mukund Varma and Peng, Nanyun and Chi, Nathan Andrew and Lee, Nayeon and Krakover, Neta Gur-Ari and Cameron, Nicholas and Roberts, Nicholas and Doiron, Nick and Martinez, Nicole and Nangia, Nikita and Deckers, Niklas and Muennighoff, Niklas and Keskar, Nitish Shirish and Iyer, Niveditha S. and Constant, Noah and Fiedel, Noah and Wen, Nuan and Zhang, Oliver and Agha, Omar and Elbaghdadi, Omar and Levy, Omer and Evans, Owain and Casares, Pablo Antonio Moreno and Doshi, Parth and Fung, Pascale and Liang, Paul Pu and Vicol, Paul and Alipoormolabashi, Pegah and Liao, Peiyuan and Liang, Percy and Chang, Peter W and Eckersley, Peter and Htut, Phu Mon and Hwang, Pinyu and Mi{\l}kowski, Piotr and Patil, Piyush and Pezeshkpour, Pouya and Oli, Priti and Mei, Qiaozhu and Lyu, Qing and Chen, Qinlang and Banjade, Rabin and Rudolph, Rachel Etta and Gabriel, Raefer and Habacker, Rahel and Risco, Ramon and Milli{\`e}re, Rapha{\"e}l and Garg, Rhythm and Barnes, Richard and Saurous, Rif A. and Arakawa, Riku and Raymaekers, Robbe and Frank, Robert and Sikand, Rohan and Novak, Roman and Sitelew, Roman and Bras, Ronan Le and Liu, Rosanne and Jacobs, Rowan and Zhang, Rui and Salakhutdinov, Russ and Chi, Ryan Andrew and Lee, Seungjae Ryan and Stovall, Ryan and Teehan, Ryan and Yang, Rylan and Singh, Sahib and Mohammad, Saif M. and Anand, Sajant and Dillavou, Sam and Shleifer, Sam and Wiseman, Sam and Gruetter, Samuel and Bowman, Samuel R. and Schoenholz, Samuel Stern and Han, Sanghyun and Kwatra, Sanjeev and Rous, Sarah A. and Ghazarian, Sarik and Ghosh, Sayan and Casey, Sean and Bischoff, Sebastian and Gehrmann, Sebastian and Schuster, Sebastian and Sadeghi, Sepideh and Hamdan, Shadi and Zhou, Sharon and Srivastava, Shashank and Shi, Sherry and Singh, Shikhar and Asaadi, Shima and Gu, Shixiang Shane and Pachchigar, Shubh and Toshniwal, Shubham and Upadhyay, Shyam and Debnath, Shyamolima Shammie and Shakeri, Siamak and Thormeyer, Simon and Melzi, Simone and Reddy, Siva and Makini, Sneha Priscilla and Lee, Soo-Hwan and Torene, Spencer and Hatwar, Sriharsha and Dehaene, Stanislas and Divic, Stefan and Ermon, Stefano and Biderman, Stella and Lin, Stephanie and Prasad, Stephen and Piantadosi, Steven and Shieber, Stuart and Misherghi, Summer and Kiritchenko, Svetlana and Mishra, Swaroop and Linzen, Tal and Schuster, Tal and Li, Tao and Yu, Tao and Ali, Tariq and Hashimoto, Tatsunori and Wu, Te-Lin and Desbordes, Th{\'e}o and Rothschild, Theodore and Phan, Thomas and Wang, Tianle and Nkinyili, Tiberius and Schick, Timo and Kornev, Timofei and Tunduny, Titus and Gerstenberg, Tobias and Chang, Trenton and Neeraj, Trishala and Khot, Tushar and Shultz, Tyler and Shaham, Uri and Misra, Vedant and Demberg, Vera and Nyamai, Victoria and Raunak, Vikas and Ramasesh, Vinay Venkatesh and vinay uday prabhu and Padmakumar, Vishakh and Srikumar, Vivek and Fedus, William and Saunders, William and Zhang, William and Vossen, Wout and Ren, Xiang and Tong, Xiaoyu and Zhao, Xinran and Wu, Xinyi and Shen, Xudong and Yaghoobzadeh, Yadollah and Lakretz, Yair and Song, Yangqiu and Bahri, Yasaman and Choi, Yejin and Yang, Yichi and Hao, Sophie and Chen, Yifu and Belinkov, Yonatan and Hou, Yu and Hou, Yufang and Bai, Yuntao and Seid, Zachary and Zhao, Zhuoye and Wang, Zijian and Wang, Zijie J. and Wang, Zirui and Wu, Ziyi}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Transactions on Machine Learning Research}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2835-8856}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=uyTL5Bvosj}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ML for Health</abbr> <figure> <picture> <img src="/assets/img/publication_preview/comparing.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="comparing.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="balagopalan2021comparing" class="col-sm-8"> <div class="title">Comparing pre-trained and feature-based models for prediction of Alzheimer‚Äôs disease based on speech</div> <div class="author"> Aparna Balagopalan, Benjamin Eyre, Jessica Robin, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Frank Rudzicz, Jekaterina Novikova' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>Frontiers in aging neuroscience</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://doi.org/10.3389/fnagi.2021.635945" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-badge-type="2" data-badge-popover="right" data-altmetric-id="105613554"></span> <span class="__dimensions_badge_embed__" data-doi="10.3389/fnagi.2021.635945" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=C75JskwAAAAJ&amp;citation_for_view=C75JskwAAAAJ:eq2jaN3J8jMC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Introduction: Research related to the automatic detection of Alzheimer‚Äôs disease (AD) is important, given the high prevalence of AD and the high cost of traditional diagnostic methods. Since AD significantly affects the content and acoustics of spontaneous speech, natural language processing, and machine learning provide promising techniques for reliably detecting AD. There has been a recent proliferation of classification models for AD, but these vary in the datasets used, model types and training and testing paradigms. In this study, we compare and contrast the performance of two common approaches for automatic AD detection from speech on the same, well-matched dataset, to determine the advantages of using domain knowledge vs. pre-trained transfer models. Methods: Audio recordings and corresponding manually-transcribed speech transcripts of a picture description task administered to 156 demographically matched older adults, 78 with Alzheimer‚Äôs Disease (AD) and 78 cognitively intact (healthy) were classified using machine learning and natural language processing as ‚ÄúAD‚Äù or ‚Äúnon-AD.‚Äù The audio was acoustically-enhanced, and post-processed to improve quality of the speech recording as well control for variation caused by recording conditions. Two approaches were used for classification of these speech samples: (1) using domain knowledge: extracting an extensive set of clinically relevant linguistic and acoustic features derived from speech and transcripts based on prior literature, and (2) using transfer-learning and leveraging large pre-trained machine learning models: using transcript-representations that are automatically derived from state-of-the-art pre-trained language models, by fine-tuning Bidirectional Encoder Representations from Transformer (BERT)-based sequence classification models. Results: We compared the utility of speech transcript representations obtained from recent natural language processing models (i.e., BERT) to more clinically-interpretable language feature-based methods. Both the feature-based approaches and fine-tuned BERT models significantly outperformed the baseline linguistic model using a small set of linguistic features, demonstrating the importance of extensive linguistic information for detecting cognitive impairments relating to AD. We observed that fine-tuned BERT models numerically outperformed feature-based approaches on the AD detection task, but the difference was not statistically significant. Our main contribution is the observation that when tested on the same, demographically balanced dataset and tested on independent, unseen data, both domain knowledge and pretrained linguistic models have good predictive performance for detecting AD based on speech. It is notable that linguistic information alone is capable of achieving comparable, and even numerically better, performance than models including both acoustic and linguistic features here. We also try to shed light on the inner workings of the more black-box natural language processing model by performing an interpretability analysis, and find that attention weights reveal interesting patterns such as higher attribution to more important information content units in the picture description task, as well as pauses and filler words. Conclusion: This approach supports the value of well-performing machine learning and linguistically-focussed processing techniques to detect AD from speech and highlights the need to compare model performance on carefully balanced datasets, using consistent same training parameters and independent test datasets in order to determine the best performing predictive model.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">balagopalan2021comparing</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Comparing pre-trained and feature-based models for prediction of Alzheimer's disease based on speech}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Balagopalan, Aparna and Eyre, Benjamin and Robin, Jessica and Rudzicz, Frank and Novikova, Jekaterina}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Frontiers in aging neuroscience}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{13}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.3389/fnagi.2021.635945}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{635945}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Frontiers Media SA}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ML for Health</abbr> <figure> <picture> <img src="/assets/img/publication_preview/acoustic.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="acoustic.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="balagopalan21_interspeech" class="col-sm-8"> <div class="title">Comparing Acoustic-Based Approaches for Alzheimer‚Äôs Disease Detection</div> <div class="author"> Aparna Balagopalan and <em>Jekaterina Novikova</em> </div> <div class="periodical"> <em>In Interspeech 2021</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://doi.org/10.21437/Interspeech.2021-759" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2106.01555" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.21437/Interspeech.2021-759" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=C75JskwAAAAJ&amp;citation_for_view=C75JskwAAAAJ:WqliGbK-hY8C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Robust strategies for Alzheimer‚Äôs disease (AD) detection is important, given the high prevalence of AD. In this paper, we study the performance and generalizability of three approaches for AD detection from speech on the recent ADReSSo challenge dataset:1) using conventional acoustic features 2) using novel pre-trained acoustic embeddings 3) combining acoustic features and embeddings. We find that while feature-based approaches have a higher precision, classification approaches relying on the combination of embeddings and features prove to have a higher, and more balanced performance across multiple metrics of performance. Our best model, using such a combined approach, outperforms the acoustic baseline in the challenge by 2.8%.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">balagopalan21_interspeech</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Comparing Acoustic-Based Approaches for Alzheimer‚Äôs Disease Detection}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Balagopalan, Aparna and Novikova, Jekaterina}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Interspeech 2021}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3800--3804}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.21437/Interspeech.2021-759}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2958-1796}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NLG</abbr> <figure> <picture> <img src="/assets/img/publication_preview/e2e_nlg.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="e2e_nlg.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10.1016/j.csl.2019.06.009" class="col-sm-8"> <div class="title">Evaluating the state-of-the-art of End-to-End Natural Language Generation: The E2E NLG challenge</div> <div class="author"> Ond≈ôej Du≈°ek, <em>Jekaterina Novikova</em>, and Verena Rieser </div> <div class="periodical"> <em>Comput. Speech Lang.</em>, Jan 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://doi.org/10.1016/j.csl.2019.06.009" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/1901.07931" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.csl.2019.06.009" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=C75JskwAAAAJ&amp;citation_for_view=C75JskwAAAAJ:5ugPr518TE4C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>This paper provides a comprehensive analysis of the first shared task on End-to-End Natural Language Generation (NLG) and identifies avenues for future research based on the results. This shared task aimed to assess whether recent end-to-end NLG systems can generate more complex output by learning from datasets containing higher lexical richness, syntactic complexity and diverse discourse phenomena. Introducing novel automatic and human metrics, we compare 62 systems submitted by 17 institutions, covering a wide range of approaches, including machine learning architectures ‚Äì with the majority implementing sequence-to-sequence models (seq2seq) ‚Äì as well as systems based on grammatical rules and templates. Seq2seq-based systems have demonstrated a great potential for NLG in the challenge. We find that seq2seq systems generally score high in terms of word-overlap metrics and human evaluations of naturalness ‚Äì with the winning Slug system (Juraska et al., 2018) being seq2seq-based. However, vanilla seq2seq models often fail to correctly express a given meaning representation if they lack a strong semantic control mechanism applied during decoding. Moreover, seq2seq models can be outperformed by hand-engineered systems in terms of overall quality, as well as complexity, length and diversity of outputs. This research has influenced, inspired and motivated a number of recent studies outwith the original competition, which we also summarise as part of this paper.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">10.1016/j.csl.2019.06.009</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Du\v{s}ek, Ond\v{r}ej and Novikova, Jekaterina and Rieser, Verena}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Evaluating the state-of-the-art of End-to-End Natural Language Generation: The E2E NLG challenge}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">issue_date</span> <span class="p">=</span> <span class="s">{Jan 2020}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Academic Press Ltd.}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{GBR}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{59}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{C}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0885-2308}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1016/j.csl.2019.06.009}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.csl.2019.06.009}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Comput. Speech Lang.}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jan</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{123‚Äì156}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{34}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ML for Health</abbr> <figure> <picture> <img src="/assets/img/publication_preview/bert_ad.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="bert_ad.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="balagopalan20_interspeech" class="col-sm-8"> <div class="title">To BERT or not to BERT: Comparing Speech and Language-Based Approaches for Alzheimer‚Äôs Disease Detection</div> <div class="author"> Aparna Balagopalan, Benjamin Eyre, Frank Rudzicz, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Jekaterina Novikova' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In Interspeech 2020</em>, Jan 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://doi.org/10.21437/Interspeech.2020-2557" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2008.01551" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.21437/Interspeech.2020-2557" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=C75JskwAAAAJ&amp;citation_for_view=C75JskwAAAAJ:_B80troHkn4C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Research related to automatically detecting Alzheimer‚Äôs disease (AD) is important, given the high prevalence of AD and the high cost of traditional methods. Since AD significantly affects the content and acoustics of spontaneous speech, natural language processing and machine learning provide promising techniques for reliably detecting AD. We compare and contrast the performance of two such approaches for AD detection on the recent ADReSS challenge dataset: 1) using domain knowledge-based hand-crafted features that capture linguistic and acoustic phenomena, and 2) fine-tuning Bidirectional Encoder Representations from Transformer (BERT)-based sequence classification models. We also compare multiple feature-based regression models for a neuropsychological score task in the challenge. We observe that fine-tuned BERT models, given the relative importance of linguistics in cognitive impairment detection, outperform feature-based approaches on the AD detection task.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">balagopalan20_interspeech</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{To BERT or not to BERT: Comparing Speech and Language-Based Approaches for Alzheimer‚Äôs Disease Detection}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Balagopalan, Aparna and Eyre, Benjamin and Rudzicz, Frank and Novikova, Jekaterina}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Interspeech 2020}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2167--2171}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.21437/Interspeech.2020-2557}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2958-1796}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ML for Health</abbr> <figure> <picture> <img src="/assets/img/publication_preview/cn_ad.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="cn_ad.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhu-etal-2019-detecting" class="col-sm-8"> <div class="title">Detecting cognitive impairments by agreeing on interpretations of linguistic features</div> <div class="author"> Zining Zhu, <em>Jekaterina Novikova</em>, and Frank Rudzicz </div> <div class="periodical"> <em>In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, Jun 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://doi.org/10.18653/v1/N19-1146" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/1808.06570" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.18653/v1/N19-1146" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=C75JskwAAAAJ&amp;citation_for_view=C75JskwAAAAJ:1qzjygNMrQYC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Linguistic features have shown promising applications for detecting various cognitive impairments. To improve detection accuracies, increasing the amount of data or the number of linguistic features have been two applicable approaches. However, acquiring additional clinical data can be expensive, and hand-crafting features is burdensome. In this paper, we take a third approach, proposing Consensus Networks (CNs), a framework to classify after reaching agreements between modalities. We divide linguistic features into non-overlapping subsets according to their modalities, and let neural networks learn low-dimensional representations that agree with each other. These representations are passed into a classifier network. All neural networks are optimized iteratively. In this paper, we also present two methods that improve the performance of CNs. We then present ablation studies to illustrate the effectiveness of modality division. To understand further what happens in CNs, we visualize the representations during training. Overall, using all of the 413 linguistic features, our models significantly outperform traditional classifiers, which are used by the state-of-the-art papers.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhu-etal-2019-detecting</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Detecting cognitive impairments by agreeing on interpretations of linguistic features}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhu, Zining and Novikova, Jekaterina and Rudzicz, Frank}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Burstein, Jill and Doran, Christy and Solorio, Thamar}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Minneapolis, Minnesota}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/N19-1146/}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/N19-1146}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1431--1441}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NLG</abbr> <figure> <picture> <img src="/assets/img/publication_preview/e2e_nlg_find.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="e2e_nlg_find.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="dusek-etal-2018-findings" class="col-sm-8"> <div class="title">Findings of the E2E NLG Challenge</div> <div class="author"> Ond≈ôej Du≈°ek, <em>Jekaterina Novikova</em>, and Verena Rieser </div> <div class="periodical"> <em>In Proceedings of the 11th International Conference on Natural Language Generation</em>, Nov 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://doi.org/10.18653/v1/W18-6539" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/1810.01170" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.18653/v1/W18-6539" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=C75JskwAAAAJ&amp;citation_for_view=C75JskwAAAAJ:J-pR_7NvFogC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>This paper summarises the experimental setup and results of the first shared task on end-to-end (E2E) natural language generation (NLG) in spoken dialogue systems. Recent end-to-end generation systems are promising since they reduce the need for data annotation. However, they are currently limited to small, delexicalised datasets. The E2E NLG shared task aims to assess whether these novel approaches can generate better-quality output by learning from a dataset containing higher lexical richness, syntactic complexity and diverse discourse phenomena. We compare 62 systems submitted by 17 institutions, covering a wide range of approaches, including machine learning architectures ‚Äì with the majority implementing sequence-to-sequence models (seq2seq) ‚Äì as well as systems based on grammatical rules and templates.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">dusek-etal-2018-findings</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Findings of the {E}2{E} {NLG} Challenge}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Du{\v{s}}ek, Ond{\v{r}}ej and Novikova, Jekaterina and Rieser, Verena}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Krahmer, Emiel and Gatt, Albert and Goudbeek, Martijn}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 11th International Conference on Natural Language Generation}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Tilburg University, The Netherlands}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/W18-6539/}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/W18-6539}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{322--328}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NLG</abbr> <figure> <picture> <img src="/assets/img/publication_preview/rankme.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="rankme.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="novikova-etal-2018-rankme" class="col-sm-8"> <div class="title">RankME: Reliable Human Ratings for Natural Language Generation</div> <div class="author"> <em>Jekaterina Novikova</em>, Ond≈ôej Du≈°ek, and Verena Rieser </div> <div class="periodical"> <em>In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)</em>, Jun 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://doi.org/10.18653/v1/N18-2012" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/1803.05928" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.18653/v1/N18-2012" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=C75JskwAAAAJ&amp;citation_for_view=C75JskwAAAAJ:mvPsJ3kp5DgC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Human evaluation for natural language generation (NLG) often suffers from inconsistent user ratings. While previous research tends to attribute this problem to individual user preferences, we show that the quality of human judgements can also be improved by experimental design. We present a novel rank-based magnitude estimation method (RankME), which combines the use of continuous scales and relative assessments. We show that RankME significantly improves the reliability and consistency of human ratings compared to traditional evaluation methods. In addition, we show that it is possible to evaluate NLG systems according to multiple, distinct criteria, which is important for error analysis. Finally, we demonstrate that RankME, in combination with Bayesian estimation of system quality, is a cost-effective alternative for ranking multiple NLG systems.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">novikova-etal-2018-rankme</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{R}ank{ME}: Reliable Human Ratings for Natural Language Generation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Novikova, Jekaterina and Du{\v{s}}ek, Ond{\v{r}}ej and Rieser, Verena}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Walker, Marilyn and Ji, Heng and Stent, Amanda}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New Orleans, Louisiana}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/N18-2012/}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/N18-2012}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{72--78}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2017</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Eval</abbr> <figure> <picture> <img src="/assets/img/publication_preview/nlg_eval.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="nlg_eval.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="novikova-etal-2017-need" class="col-sm-8"> <div class="title">Why We Need New Evaluation Metrics for NLG</div> <div class="author"> <em>Jekaterina Novikova</em>, Ond≈ôej Du≈°ek, Amanda Cercas Curry, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Verena Rieser' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</em>, Sep 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://doi.org/10.18653/v1/D17-1238" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/1707.06875" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.18653/v1/D17-1238" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=C75JskwAAAAJ&amp;citation_for_view=C75JskwAAAAJ:eJXPG6dFmWUC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>The majority of NLG evaluation relies on automatic metrics, such as BLEU . In this paper, we motivate the need for novel, system- and data-independent automatic evaluation methods: We investigate a wide range of metrics, including state-of-the-art word-based and novel grammar-based ones, and demonstrate that they only weakly reflect human judgements of system outputs as generated by data-driven, end-to-end NLG. We also show that metric performance is data- and system-specific. Nevertheless, our results also suggest that automatic metrics perform reliably at system-level and can support system development by finding cases where a system performs poorly.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">novikova-etal-2017-need</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Why We Need New Evaluation Metrics for {NLG}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Novikova, Jekaterina and Du{\v{s}}ek, Ond{\v{r}}ej and Cercas Curry, Amanda and Rieser, Verena}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Palmer, Martha and Hwa, Rebecca and Riedel, Sebastian}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Copenhagen, Denmark}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/D17-1238/}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/D17-1238}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2241--2252}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NLG</abbr> <figure> <picture> <img src="/assets/img/publication_preview/e2e_new_chal.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="e2e_new_chal.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="novikova-etal-2017-e2e" class="col-sm-8"> <div class="title">The E2E Dataset: New Challenges For End-to-End Generation</div> <div class="author"> <em>Jekaterina Novikova</em>, Ond≈ôej Du≈°ek, and Verena Rieser </div> <div class="periodical"> <em>In Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue</em>, Aug 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">üèÜ Awarded</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://doi.org/10.18653/v1/W17-5525" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/1706.09254" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.18653/v1/W17-5525" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=C75JskwAAAAJ&amp;citation_for_view=C75JskwAAAAJ:VOx2b1Wkg3QC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Best Paper Nomination at SIGDIAL 2017</p> </div> <div class="abstract hidden"> <p>This paper describes the E2E data, a new dataset for training end-to-end, data-driven natural language generation systems in the restaurant domain, which is ten times bigger than existing, frequently used datasets in this area. The E2E dataset poses new challenges: (1) its human reference texts show more lexical richness and syntactic variation, including discourse phenomena; (2) generating from this set requires content selection. As such, learning from this dataset promises more natural, varied and less template-like system utterances. We also establish a baseline on this dataset, which illustrates some of the difficulties associated with this data.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">novikova-etal-2017-e2e</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{The {E}2{E} Dataset: New Challenges For End-to-End Generation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Novikova, Jekaterina and Du{\v{s}}ek, Ond{\v{r}}ej and Rieser, Verena}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Jokinen, Kristiina and Stede, Manfred and DeVault, David and Louis, Annie}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 18th Annual {SIG}dial Meeting on Discourse and Dialogue}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">aug</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Saarbr{\"u}cken, Germany}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/W17-5525/}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/W17-5525}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{201--206}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2016</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NLG</abbr> <figure> <picture> <img src="/assets/img/publication_preview/crowd_nlg.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="crowd_nlg.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="novikova-etal-2016-crowd" class="col-sm-8"> <div class="title">Crowd-sourcing NLG Data: Pictures Elicit Better Data.</div> <div class="author"> <em>Jekaterina Novikova</em>, Oliver Lemon, and Verena Rieser </div> <div class="periodical"> <em>In Proceedings of the 9th International Natural Language Generation conference</em>, Sep 2016 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://doi.org/10.18653/v1/W16-6644" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/1608.00339" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.18653/v1/W16-6644" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=C75JskwAAAAJ&amp;citation_for_view=C75JskwAAAAJ:B3FOqHPlNUQC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Recent advances in corpus-based Natural Language Generation (NLG) hold the promise of being easily portable across domains, but require costly training data, consisting of meaning representations (MRs) paired with Natural Language (NL) utterances. In this work, we propose a novel framework for crowdsourcing high quality NLG training data, using automatic quality control measures and evaluating different MRs with which to elicit data. We show that pictorial MRs result in better NL data being collected than logic-based MRs: utterances elicited by pictorial MRs are judged as significantly more natural, more informative, and better phrased, with a significant increase in average quality ratings (around 0.5 points on a 6-point scale), compared to using the logical MRs. As the MR becomes more complex, the benefits of pictorial stimuli increase. The collected data will be released as part of this submission.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">novikova-etal-2016-crowd</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Crowd-sourcing {NLG} Data: Pictures Elicit Better Data.}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Novikova, Jekaterina and Lemon, Oliver and Rieser, Verena}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Isard, Amy and Rieser, Verena and Gkatzia, Dimitra}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 9th International Natural Language Generation conference}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2016}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Edinburgh, UK}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/W16-6644/}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/W16-6644}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{265--273}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2015</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">HRI</abbr> <figure> <picture> <img src="/assets/img/publication_preview/art_emot.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="art_emot.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="novikova2015towards" class="col-sm-8"> <div class="title">Towards Artificial Emotions to Assist Social Coordination in HRI</div> <div class="author"> <em>Jekaterina Novikova</em> and Leon Watts </div> <div class="periodical"> <em>International Journal of Social Robotics</em>, Sep 2015 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://doi.org/10.1007/s12369-014-0254-y" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://purehost.bath.ac.uk/ws/portalfiles/portal/157913058/ArtEmotionsInRobotsv5.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1007/s12369-014-0254-y" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=C75JskwAAAAJ&amp;citation_for_view=C75JskwAAAAJ:u9iWguZQMMsC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Coordination of human‚Äìrobot joint activity must depend on the ability of human and artificial agencies to interpret and interleave their actions. In this paper we consider the potential of artificial emotions to serve as task-relevant coordination devices in human‚Äìrobot teams. We present two studies aiming to understand whether a non-humanoid robot can express artificial emotions in a manner that is meaningful to a human observer, the first based on static images and the second on the dynamic production of embodied robot expressions. We present a mixed-methods approach to the problem, combining statistical treatment of ratings data and thematic analysis of qualitative data. Our results demonstrate that even very simple movements of a non-humanoid robot can convey emotional meaning, and that when people attribute emotional states to a robot, they typically apply an event-based frame to make sense of the robotic expressions they have seen. Artificial emotions with high arousal level and negative valence are relatively easy for people to recognise compared to expressions with positive valence. We discuss the potential for using motion in different parts of a non-humanoid robot body to support the attribution of emotion in HRI, towards ethically responsible design of artificial emotions that could contribute to the efficacy of joint human‚Äìrobot activities.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">novikova2015towards</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Towards Artificial Emotions to Assist Social Coordination in HRI}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Novikova, Jekaterina and Watts, Leon}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Journal of Social Robotics}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{7}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{77--88}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2015}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1007/s12369-014-0254-y}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2014</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">HRI</abbr> <figure> <picture> <img src="/assets/img/publication_preview/robot_emot.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="robot_emot.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="novikova2014design" class="col-sm-8"> <div class="title">A Design Model of Emotional Body Expressions in Non-humanoid Robots</div> <div class="author"> <em>Jekaterina Novikova</em> and Leon Watts </div> <div class="periodical"> <em>In Proceedings of the second international conference on Human-agent interaction</em>, Sep 2014 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">üèÜ Awarded</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://doi.org/10.1145/2658861.2658892" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://researchportal.bath.ac.uk/files/111088746/NovikovaWattsHAI2014.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1145/2658861.2658892" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=C75JskwAAAAJ&amp;citation_for_view=C75JskwAAAAJ:p2g8aNsByqUC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Best Paper nomination at HAI 2014</p> </div> <div class="abstract hidden"> <p>Robotic emotional expressions could benefit social communication between humans and robots, if the cues such expressions contain were to be intelligible to human observers. In this paper, we present a design framework for modelling emotionally expressive robotic movements. The framework combines approach-avoidance with Shape and Effort dimensions, derived from Laban, and makes use of anatomical body planes that are general to both humanoid and non-humanoid body forms. An experimental validation study is reported with 34 participants rating an implementation of five expressive behaviours on a non-humanoid robotic platform. The results demonstrate that such expressions can encode basic emotional information, in that the parameters of the proposed design model can convey the meaning of emotional dimensions of valence, arousal and dominance. The framework thus creates a basis for implementing a set of emotional expressions that are appropriately adapted to contexts of human-robot joint activity.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">novikova2014design</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Design Model of Emotional Body Expressions in Non-humanoid Robots}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Novikova, Jekaterina and Watts, Leon}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the second international conference on Human-agent interaction}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{353--360}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2014}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/2658861.2658892}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2025 Jekaterina Novikova. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-JWB4SMNB0X"></script> <script defer src="/assets/js/google-analytics-setup.js"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>